{\rtf1\ansi\ansicpg1252\cocoartf1404\cocoasubrtf470
{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\fnil\fcharset0 Monaco;\f2\fmodern\fcharset0 Courier;
\f3\fnil\fcharset0 Menlo-Regular;\f4\fnil\fcharset0 LucidaGrande;\f5\froman\fcharset0 TimesNewRomanPSMT;
\f6\fmodern\fcharset0 Courier-Bold;\f7\fswiss\fcharset0 ArialMT;\f8\fmodern\fcharset0 CourierNewPSMT;
}
{\colortbl;\red255\green255\blue255;\red0\green0\blue255;\red255\green0\blue255;\red0\green170\blue0;
\red128\green0\blue0;\red255\green0\blue0;\red201\green128\blue43;\red28\green28\blue28;\red38\green38\blue38;
\red246\green246\blue246;\red26\green26\blue26;\red22\green23\blue26;\red231\green236\blue240;\red0\green0\blue0;
\red0\green0\blue0;\red64\green11\blue217;}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid1}
{\list\listtemplateid2\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid101\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid2}
{\list\listtemplateid3\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid201\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid3}
{\list\listtemplateid4\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid301\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid4}
{\list\listtemplateid5\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid401\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid5}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}{\listoverride\listid2\listoverridecount0\ls2}{\listoverride\listid3\listoverridecount0\ls3}{\listoverride\listid4\listoverridecount0\ls4}{\listoverride\listid5\listoverridecount0\ls5}}
\margl1440\margr1440\vieww28040\viewh17060\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 \
Supervised Classification - Learning from examples.\
Supervised Learning \
\
\cf2 acerous\cf0  \
Elephant\
Lemurs\
Dog \
Cat\
Parrot\
Horse\
\
\cf2 Not acerous\cf0  \
Ram\
Triceratops \
Giraffe   \
Goat\
Deer\
\
\
\cf2 Supervised Classification big bank of data - correct answer known \cf0 \
Photographs - recognize someone in a picture - \cf2 Supervised Classification Example\cf0 \
Recommend music based on history, recommender systems  - \cf2 Supervised Classification\cf0  \
\
Bank data fraud recognition - \cf2 Unsupervised Learning Example\
\cf0 Cluster udacity students based on learning style (clustering) - \cf2 Unsupervised Learning Example\
\
\
\cf0 Features input to brain\cf2 \
music intensity\
music tempo\
music genre\
music gender\
\
\cf0 Output\
\cf2 Like Song\
Do Not Like Song\
\
\cf0 Stanley Terrain Classification\
Supervised Machine Learning \
\
scikit-learn - sklearn naive bayes \
Gaussian Naive Bayes\
\
***\
*** Introduction to Machine Learning - Naive Bayes - \cf2 Calculating Naive Bayes Accuracy \cf0  20 of 43 - 27 February 2017 \
***\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1\fs22 \cf0 \
\pard\pardeftab720\partightenfactor0
\cf3 # accuracy of GaussianNB() classifier method 1 of 3 <---------------------------------------------------------\cf0 \
\cf3 # accuracy - get the accuracy \ul og\ulnone  the classifier # 0.884\cf0 \
my_clf_score = clf.score(features_test,labels_test)\
\cf2 print\cf0 (\cf4 "\\tmy_clf_score is \{\}\\n"\cf0 .format(my_clf_score)) \cf3 # 0.884\cf0 \
\cf3 # print("\\\ul ttype\ulnone (my_clf_score) - \{\}\\n".format(type(my_clf_score))) # class 'numpy.float64'\cf0 \
\
\cf3 # accuracy of GaussianNB() classifier method 2 of 3 <---------------------------------------------------------\cf0 \
\cf3 # enumerate through \ul Python\ulnone  list, also keep track of list index \cf0 \
\cf3 # enumerate through the labels_test *** list ***\cf0 \
matchCount = \cf5 0\cf0 \
\cf2 for\cf0  idx, label \cf2 in\cf0  enumerate(labels_test):\
    \cf3 # print("\\\ul tidx\ulnone  is \{\}".format(\ul idx\ulnone )) # 0 - 249\cf0 \
    \cf3 # print("\\\ul ttype\ulnone (\ul idx\ulnone ) - \{\}\\n".format(type(\ul idx\ulnone ))) # \ul int\ulnone , \ul int\cf0 \ulnone \
    \cf3 # print("\\\ul tlabel\ulnone  is \{\}".format(label)) # 0, 1, 1.0\cf0 \
    \cf3 # print("\\\ul ttype\ulnone (label) - \{\}\\n".format(type(label))) # class '\ul int\ulnone ', class 'float'\cf0 \
    \cf3 # print("\\\ul tpred\ulnone [\ul idx\ulnone ] is \{\}".format(\ul pred\ulnone [\ul idx\ulnone ])) # 0.0, 1.0 ...\cf0 \
    \cf3 # print("\\\ul ttype\ulnone (\ul pred\ulnone [\ul idx\ulnone ]) - \{\}\\n".format(type(\ul pred\ulnone [\ul idx\ulnone ]))) # numpy.float64\cf0 \
    \cf2 if\cf0  label == pred[idx]:\
        matchCount +=\cf5 1\cf0 \
        \cf3 # print('we have a winner')\cf0 \
        \
\cf3 # accuracy - get the accuracy of the classifier # 0.884\cf0 \
\cf2 print\cf0 (\cf4 "\\\ul tlen\ulnone (labels_test) is \{\}"\cf0 .format(len(labels_test))) \cf3 # \cf0 \
\cf2 print\cf0 (\cf4 "\\tmatchCount is \{\}"\cf0 .format(matchCount)) \cf3 # \cf0 \
\cf2 print\cf0 (\cf4 "\\tmatchCount / \ul len\ulnone (labels_test) is \{\}\\n"\cf0 .format(matchCount / len(labels_test))) \cf3 # 0.884\cf0 \
\
\cf3 # sklearn.metrics.accuracy_score(\ul pred\ulnone ,labels_test), accuracy method used by instructor in video\cf0 \
\cf3 # accuracy of GaussianNB() classifier method 3 of 3 <---------------------------------------------------------\cf0 \
myGaussianNB_Classifier_Accuracy = sklearn.metrics.\cf6 accuracy_score\cf0 (pred,labels_test)\
\cf2 print\cf0 (\cf4 "\\tmyGaussianNB_Classifier_Accuracy - \{\}"\cf0 .format(myGaussianNB_Classifier_Accuracy))\
\cf2 print\cf0 (\cf4 "\\\ul ttype\ulnone (myGaussianNB_Classifier_Accuracy) - \{\}\\n"\cf0 .format(type(myGaussianNB_Classifier_Accuracy)))\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 \
\
\
***\
*** Introduction to Machine Learning - Naive Bayes - \cf2 Cancer Test\cf0  24 of 43 - 27 February 2017 \
***\
\
\pard\pardeftab720\sl280\partightenfactor0

\f2 \cf0 \expnd0\expndtw0\kerning0
Bayes Rule\
Specific Cancer Occurs in 1% (\cf6 0.01\cf0 ) of the Population\
P(C) = 0.01 - Probability of Cancer\
Cancer Test - 90% chance is it positive if you have the cancer (\cf2 sensitivity\cf0  of test)\
Cancer Test - 90% chance is it negative if you do not have the cancer (\cf2 specitivity\cf0  (\cf2 specificity\cf0 ))\
\
Prior			Test			Posterior\
Probability		Evidence		Probability\
\
Bayes Rule\
Prior Probability OF Having The Cancer - (Before you run the Test) - \cf6 0.01\cf0 \
\
\cf6 \ul \ulc6 PRIOR\cf0 \ulnone \
Probability Of Having Cancer = \cf2 0.01\cf0 \
Probability Of Positive Test with Cancer = \cf2 0.9\cf0 \
Probability Of Cancer With Positive Test = \cf2 0.01\cf0  * \cf2 0.9\cf0  = \cf6 0.009\cf0 \

\f0 \kerning1\expnd0\expndtw0 \

\f2 \expnd0\expndtw0\kerning0
Probability Of Not Having Cancer = \cf2 0.99\cf0 \
Probability Of Positive Test with NO Cancer = \cf2 0.1\cf0 \
Probability Of Positive Test NO Cancer = \cf6 0.099\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0 \cf6 \kerning1\expnd0\expndtw0 Calculate Posterior Probabilities By Normalizing
\f2 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720\sl280\partightenfactor0
\cf2 Calculate Normalizer \cf6 \
0.009 \cf2 +\cf6  0.099 = \cf2 0.108\
\
Joint Probability Of Two Events\
\cf0 Probability Of Cancer With Positive Test = \cf2 0.01\cf0  * \cf2 0.9\cf0  = \cf6 0.009\
\cf0 Probability Of Positive Test NO Cancer = \cf6 0.099\
\cf0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0 \cf6 \kerning1\expnd0\expndtw0 Calculate Posterior Probabilities By Normalizing
\f2 \cf0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720\sl280\partightenfactor0
\cf2 \ul \ulc2 Divide by Normalizer - Calculate Posterior Probability \cf0 \ulnone \
Normalized Probability Has Cancer with a positive test result - \cf6 0.009 / \cf2 0.108 = 0.0833 \cf0 (\cf2 posterior\cf0 )\
Normalized Probability NO Cancer  with a positive test result - \cf6 0.099 / \cf2 0.108 = 0.9167 \cf0 (\cf2 posterior\cf0 )\cf2 \
\
0.08333 + 0.9167 = \cf6 1 ** adding up to 1 big deal **\cf0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0 \cf2 \kerning1\expnd0\expndtw0 \
\
\cf0 ***\
*** Introduction to Machine Learning - Naive Bayes - \cf2 Bayes Rule for Classification \cf0  31 of 43 - 27 February 2017 \
*** Introduction to Machine Learning - Naive Bayes - \cf2 Chris or Sara \cf0  32 of 43 - 27 February 2017\
***\
\
\pard\pardeftab720\sl280\partightenfactor0

\f2 \cf0 \expnd0\expndtw0\kerning0
Prior - even odds email is from Chris or Sara\
p(Chris)	= 0.5\
p(Sara)	= 0.5\
\
Chris - \cf2 given data\cf0 \
Love - 0.1 - Deal - 0.8 - Life 0.1\
\
Sara - \cf2 given data\cf0 \
Love - 0.5 - Deal - 0.2 - Life 0.3\
\
Email says - Life Deal - slightly more likely email is from Chris\
Chris	- usage of Life probability(0.1) * usage of Deal probability(0.8) * Prior(0.5) = \cf6 0.04 ** Chris by a SLIM margin **\cf0 \
Sara		- usage of Life probability(0.3) * usage of Deal probability(0.2) * Prior(0.5) = \cf6 0.03\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0 \cf0 \kerning1\expnd0\expndtw0 ***\
*** Introduction to Machine Learning - Naive Bayes - \cf2 Posterior Possibilities \cf0  33 of 43 - 28 February 2017 \
***\
\
\pard\pardeftab720\sl280\partightenfactor0

\f2 \cf0 \expnd0\expndtw0\kerning0
Prior - even odds email is from Chris or Sara\
p(Chris)	= 0.5\
p(Sara)	= 0.5\
\
Chris - \cf2 given data\cf0 \
Love - 0.1 - Deal - 0.8 - Life 0.1\
\
Sara - \cf2 given data\cf0 \
Love - 0.5 - Deal - 0.2 - Life 0.3\
\
Email says - Life Deal - slightly more likely email is from Chris\
Chris	- usage of Life probability(0.1) * usage of Deal probability(0.8) * Prior(0.5) = \cf6 0.04 ** Chris by a SLIM margin **\cf0 \
Sara		- usage of Life probability(0.3) * usage of Deal probability(0.2) * Prior(0.5) = \cf6 0.03\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0 \cf0 \kerning1\expnd0\expndtw0 \
\
Probability email is from Chris - \cf6 0.04\cf0 \
Probability email is from Sara - \cf6 0.03\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf6 \ul ** \ulnone Calculate Posterior Probabilities By Normalizing\ul  **\ulnone \
\pard\pardeftab720\sl280\partightenfactor0

\f2 \cf0 \expnd0\expndtw0\kerning0
1 of 2 - \cf2 First Calculate Normalizer \cf6 \
0.04 \cf2 +\cf6  0.03 = \cf2 0.07\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0 \cf0 \kerning1\expnd0\expndtw0 2 of 2 - \cf6 ** Calculate Posterior Probabilities By Normalizing **\cf0  - 
\f2 \cf2 \expnd0\expndtw0\kerning0
Divide by Normalizer\cf0 \
\pard\pardeftab720\sl280\partightenfactor0
\cf0 Normalized Probability email is from Chris	- \cf6 0.04 / \cf2 0.07 = 0.571 \cf0 (posterior)\
Normalized Probability email is from Sara	- \cf6 0.03 / \cf2 0.07 = 0.429 \cf0 (posterior) \
\
0.571 + 0.429 = 1 ** \cf2 equals 1 - big deal\cf0  **\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0 \cf0 \kerning1\expnd0\expndtw0 ***\
*** Introduction to Machine Learning - Naive Bayes - \cf2 Bayesian Possibilities On Your Own \cf0  34 of 43 - 28 February 2017 \
***\
\pard\pardeftab720\sl280\partightenfactor0

\f2 \cf0 \expnd0\expndtw0\kerning0
\
Email says - Love Deal - slightly more likely email is from Chris\
Chris	- usage of Love probability(0.1) * usage of Deal probability(0.8) * Prior(0.5), 0.1 * 0.8 * 0.5 = \cf6 0.04\cf0 \
Sara		- usage of Love probability(0.5) * usage of Deal probability(0.2) * Prior(0.5), 0.5 * 0.2 * 0.5 = \cf6 0.05 ** Sara by a SLIM margin **\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0 \cf0 \kerning1\expnd0\expndtw0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf6 \ul \ulc6 ** Calculate Posterior Probabilities By Normalizing ** \ulnone \
\pard\pardeftab720\sl280\partightenfactor0

\f2 \cf0 \expnd0\expndtw0\kerning0
1 of 2 - \cf2 First Calculate Normalizer \cf6 \
0.04 \cf2 +\cf6  0.05 = \cf2 0.09\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0 \cf0 \kerning1\expnd0\expndtw0 2 of 2 - \cf6 ** Calculate Posterior Probabilities By Normalizing **\cf0  - 
\f2 \cf2 \expnd0\expndtw0\kerning0
Divide by Normalizer\cf0 \
\pard\pardeftab720\sl280\partightenfactor0
\cf0 Normalized Probability email is from Chris	- \cf6 0.04 / \cf2 0.09 = 0.444 \cf0 (posterior)\
Normalized Probability email is from Sara	- \cf6 0.05 / \cf2 0.09 = 0.555 \cf0 (posterior) \
\
0.444 + 0.555 = 0.999  ** \cf2 equals 1 - big deal\cf0  **\
\
Naive Bayes - Frequently used with text, who wrote this Shakespeare, Chris, Sara ... \
Naive Bayes - Called "naive" because it does take into account word order - just the frequency of the words\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0 \cf0 \kerning1\expnd0\expndtw0 ***\
*** Introduction to Machine Learning - Naive Bayes - \cf2 Getting Started with Mini-Projects \cf0 39 of 43 -  February 2017 \
***
\f2 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720\sl280\partightenfactor0
\cf0 \
Support Vector Machines \
\
Welcome to the first mini-project! Here's what to expect:\
you'll download the project starter files (one time only)\
you'll run a startup script\
near the end of each lesson, there will be a video and/or reading node telling you about the mini-project for that lesson\
then, a series of reading nodes at the end of the lesson will walk you through what you should do for that mini-project, and give you a chance to enter the answers that you get as you work through the tasks (these will look like quizzes)\
you'll develop the code on your own computer based on the instructions in the reading nodes\
the code you write will enable you to answer the quiz questions\
\
So, what now?\
if you know git, you can clone the starter files: git clone \cf2 https://github.com/udacity/ud120-projects.git\cf0 \
If you don't know git, Udacity has a great (and short) course that can get you started\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0 \cf0 \kerning1\expnd0\expndtw0 ***\
*** Introduction to Machine Learning - Naive Bayes - \cf2 Machine learning for Author ID \cf0 40 of 43 -  February 2017 \
***\
\
We\'92ll do something very similar in this project. We have a set of emails, half of which were written by one person and the other half by another person at the same company .\
Our objective is to classify the emails as written by one person or the other based only on the text of the email.\
We will start with Naive Bayes in this mini-project, and then expand in later projects to other algorithms.\
\
We will start by giving you a list of strings. Each string is the text of an email, which has undergone some basic preprocessing; we will then provide the code to split the dataset into training and testing sets. (In the next lessons you\'92ll learn how to do this preprocessing and splitting yourself, but for now we\'92ll give the code to you).\
\
One particular feature of Naive Bayes is that it\'92s a good algorithm for working with text classification. \
When dealing with text, it\'92s very common to treat each unique word as a feature, and since the typical person\'92s vocabulary is many thousands of words, this makes for a large number of features. \
The relative simplicity of the algorithm and the independent features assumption of Naive Bayes make it a strong performer for classifying texts. \
In this mini-project, you will download and install sklearn on your computer and use Naive Bayes to classify emails by author.\
\pard\pardeftab720\sl280\partightenfactor0

\f2 \cf0 \expnd0\expndtw0\kerning0
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardeftab720\pardirnatural\partightenfactor0

\f0 \cf0 \kerning1\expnd0\expndtw0 ***\
*** Introduction to Machine Learning - Naive Bayes - \cf2 Getting Your Code Setup  \cf0 41 of 43 -  February 2017 \
***\
Check that you have a working python installation, preferably version 2.6 or 2.7 (that\'92s the version that we use--other versions may work, but we can\'92t guarantee it) \
We will use pip to install some packages. First get and install pip from here.\
Using pip, install a bunch of python packages:\
go to your terminal line (don\'92t open python, just the command prompt)\
install sklearn: pip install scikit-learn\
for your reference, here\'92s the sklearn installation instructions\
install natural language toolkit: pip install nltk\
Get the Intro to Machine Learning source code. You will need git to clone the repository: git clone https://github.com/udacity/ud120-projects.git\
\
You only have to do this once, the code base contains the starter code for all the mini-projects. Go into the tools/ directory, and run startup.py. It will first check for the python modules, then download and unzip a large dataset that we will use heavily later. The download/unzipping can take a while, but you don\'92t have to wait for it to finish in order to get started on Part 1.\
\
***\
*** Introduction to Machine Learning - Naive Bayes - \cf2 Quiz: Author ID Accuracy  \cf0 41 of 43 -  February 2017 \
***\

\fs28 \cf6 Create and train a Naive Bayes classifier in naive_bayes/nb_author_id.py. Use it to make predictions for the test set. What is the accuracy?\
see nb_author_id.py\

\fs24 \cf0 \
When training you may see the following error: UserWarning: Duplicate scores. Result may depend on feature ordering.There are probably duplicate features, or you used a classification score for a regression task. warn("Duplicate scores. Result may depend on feature ordering.")\
\
This is a warning that two or more words happen to have the same usage patterns in the emails--as far as the algorithm is concerned, this means that two features are the same. Some algorithms will actually break (mathematically won\'92t work) or give multiple different answers (depending on feature ordering) when there are duplicate features and sklearn is giving us a warning. Good information, but not something we have to worry about.\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
Quiz: Author ID Accuracy\
\
Some students have encountered memory problems when executing the code for this problem. \
To reduce the chance of seeing a memory error while running the code, we recommend that you use a computer with at least 2GB of RAM. \
If you find that the code is causing memory errors, you can also try setting test_size = 0.5 in the email_preprocess.py file.\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f3\fs22 \cf0 \CocoaLigature0  \
\pard\pardeftab720\partightenfactor0

\f1 \cf0 \CocoaLigature1 classifier = GaussianNB()\
classifier.fit(features_train,labels_train) ** \cf2 fit - training features, training labels used\cf0  **\
pred = classifier.predict(features_test)
\f3 \CocoaLigature0 \
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1 \cf0 \CocoaLigature1 \
\pard\pardeftab720\partightenfactor0
\cf3 # What is the accuracy of your Naive \ul Bayes\ulnone  author identifier? 1 of 3\cf0 \
my_classifier_score = classifier.score(features_test,labels_test)\
\pard\pardeftab720\partightenfactor0
\cf2 print\cf0 (\cf7 "my_classifier_score - \{\}"\cf0 .format(my_classifier_score))\
\pard\pardeftab720\partightenfactor0
\cf3 #      my_classifier_score - 0.9732650739476678 1 of 3\cf0 \
\cf3 # print("type(my_classifier_score)- \{\}\\n".format(type(my_classifier_score)))\cf0 \
\cf3 #        type(my_classifier_score)- <class 'numpy.float64'>\cf0 \
\
\cf3 # What is the accuracy of your Naive \ul Bayes\ulnone  author identifier? 2 of 3\cf0 \
matchCount = \cf5 0\cf0 \
\pard\pardeftab720\partightenfactor0
\cf2 for\cf0  idx, label \cf2 in\cf0  enumerate(labels_test):\
    \cf3 # print("\\\ul tidx\ulnone  is \{\}".format(\ul idx\ulnone )) # 0 - 249\cf0 \
    \cf3 # print("\\\ul ttype\ulnone (\ul idx\ulnone ) - \{\}\\n".format(type(\ul idx\ulnone ))) # \ul int\ulnone , \ul int\cf0 \ulnone \
    \cf3 # print("\\\ul tlabel\ulnone  is \{\}".format(label)) # 0, 1, 1.0\cf0 \
    \cf3 # print("\\\ul ttype\ulnone (label) - \{\}\\n".format(type(label))) # class '\ul int\ulnone ', class 'float'\cf0 \
    \cf3 # print("\\\ul tpred\ulnone [\ul idx\ulnone ] is \{\}".format(\ul pred\ulnone [\ul idx\ulnone ])) # 0.0, 1.0 ...\cf0 \
    \cf3 # print("\\\ul ttype\ulnone (\ul pred\ulnone [\ul idx\ulnone ]) - \{\}\\n".format(type(\ul pred\ulnone [\ul idx\ulnone ]))) # numpy.float64\cf0 \
    \cf2 if\cf0  label == pred[idx]:\
        matchCount +=\cf5 1\cf0 \
        \
\cf2 print\cf0 (\cf7 "\ul len\ulnone (labels_test) is \{\}"\cf0 .format(len(labels_test))) \
\pard\pardeftab720\partightenfactor0
\cf3 #      \ul len\ulnone (labels_test) is 1758\cf0 \
\pard\pardeftab720\partightenfactor0
\cf2 print\cf0 (\cf7 "matchCount is \{\}"\cf0 .format(matchCount)) \
\pard\pardeftab720\partightenfactor0
\cf3 #      matchCount is 1711\cf0 \
\pard\pardeftab720\partightenfactor0
\cf2 print\cf0 (\cf7 "matchCount / \ul len\ulnone (labels_test) is \{\}\\n"\cf0 .format(matchCount / len(labels_test)))\
\pard\pardeftab720\partightenfactor0
\cf3 #      matchCount / \ul len\ulnone (labels_test) is 0.9732650739476678\cf0 \
\
\cf3 # sklearn.metrics.accuracy_score(\ul pred\ulnone ,labels_test), accuracy method used by instructor in video\cf0 \
\cf3 # What is the accuracy of your Naive \ul Bayes\ulnone  author identifier? 3 of 3\cf0 \
myGaussianNB_Classifier_Accuracy = sklearn.metrics.\cf6 accuracy_score\cf0 (pred,labels_test)\
\pard\pardeftab720\partightenfactor0
\cf2 print\cf0 (\cf7 "myGaussianNB_Classifier_Accuracy - \{\}\\n"\cf0 .format(myGaussianNB_Classifier_Accuracy))\
\pard\pardeftab720\partightenfactor0
\cf3 #      myGaussianNB_Classifier_Accuracy - 0.9732650739476678\cf0 \
\cf3 # print("type(myGaussianNB_Classifier_Accuracy) - \{\}\\n".format(type(myGaussianNB_Classifier_Accuracy)))\cf0 \
\cf3 #      type(myGaussianNB_Classifier_Accuracy) - <class 'numpy.float64'>\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardeftab720\pardirnatural\partightenfactor0

\f0\fs24 \cf0 ***\
*** Introduction to Machine Learning - Naive Bayes - \cf2 Timing Your NB Classifier   \cf0 43 of 43 -  3 March 2017 \
***\
\
timing, (time), (performance), timing Gaussian Naive Bayes (GaussianNB) classifier training and prediction\
\
\pard\pardeftab720\partightenfactor0

\f1\fs22 \cf0 t0 = time() ** ** \
classifier.fit(features_train,labels_train) ** \cf2 fit - training features, training labels used\cf0  **\
\cf2 print\cf0 (\cf7 "training time: - \{\}"\cf0 .format(round(time() - t0, \cf5 3\cf0 )))\
\pard\pardeftab720\partightenfactor0
\cf3 # print "training time:", round(time()-t0, 3), "s"\cf0 \
\
t0 = time()\
pred = classifier.predict(features_test)\
\pard\pardeftab720\partightenfactor0
\cf2 print\cf0 (\cf7 "predict time: - \{\}\\n"\cf0 .format(round(time() - t0, \cf5 3\cf0 )))\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\pard\pardeftab720\partightenfactor0
\cf0 training time: - 1.213\
predict time: - 0.169 - prediction is faster than training, less time\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardeftab720\pardirnatural\partightenfactor0

\f0\fs24 \cf0 ***\
*** Introduction to Machine Learning - Naive Bayes - \cf2 SVM Support Vector Machines    \cf0 NN of 37 -  5 March 2017 \
***\
SVM - Support Vector Machines (actually an algorithm, not a machine), \cf2 very popular algorithm\
\cf0 Find separating line hyperplane, decision boundary (per instructor "\cf6 perfectly straight\cf0 ") between two classes.\
Best line maximizes the distance to the point representing the two classes - the distance is called the margin\
Margin is the distance between the separating line, hyperplane, decision boundary and the nearest point of the two classes.\
Margin line maximizes distance to the nearest point of either of the two classes.\
Goal maximize robustness of result.  \
*** \cf6 Making the CORRECT CLASSIFICATION takes PRECEDENCE over MAXIMIZING the MARGIN, \cf2 when determining the separating line or the hyperplane.\cf6  \cf0  ***\
\
\
\pard\pardeftab720\partightenfactor0

\f1\fs22 \cf0 ** \cf2 Create, Instantiate, Naive Bayes Gaussian Terrain Classifier\cf0   **\
\cf2 from\cf0  sklearn.naive_bayes \cf2 import\cf0  GaussianNB\
clf = GaussianNB()\
\
# Train - Naive Bayes Gaussian Terrain Classifier ** learns patterns ** \
clf.fit(features_train, labels_train) ** \cf2 fit - training features, training labels used\cf0  **\
\
# create \cf6 predict\cf0  - Naive Bayes Gaussian Terrain Classifier\
pred = clf.predict(features_test)\
\
# Naive Bayes Gaussian Terrain Classifier - ACCURACY 1 of 3 \cf6 # 0.884\cf0 \
my_clf_score = clf.score(features_test,labels_test)\
\
# Naive Bayes Gaussian Terrain Classifier - ACCURACY 2 of 3 \cf6 # 0.884\cf0 \
matchCount = \cf5 0\cf0 \
\cf2 for\cf0  idx, label \cf2 in\cf0  enumerate(labels_test):\
    \cf3 # print("\\\ul tidx\ulnone  is \{\}".format(\ul idx\ulnone )) # 0 - 249\cf0 \
    \cf3 # print("\\\ul ttype\ulnone (\ul idx\ulnone ) - \{\}\\n".format(type(\ul idx\ulnone ))) # \ul int\ulnone , \ul int\cf0 \ulnone \
    \cf3 # print("\\\ul tlabel\ulnone  is \{\}".format(label)) # 0, 1, 1.0\cf0 \
    \cf3 # print("\\\ul ttype\ulnone (label) - \{\}\\n".format(type(label))) # class '\ul int\ulnone ', class 'float'\cf0 \
    \cf3 # print("\\\ul tpred\ulnone [\ul idx\ulnone ] is \{\}".format(\ul pred\ulnone [\ul idx\ulnone ])) # 0.0, 1.0 ...\cf0 \
    \cf3 # print("\\\ul ttype\ulnone (\ul pred\ulnone [\ul idx\ulnone ]) - \{\}\\n".format(type(\ul pred\ulnone [\ul idx\ulnone ]))) # numpy.float64\cf0 \
    \cf2 if\cf0  label == pred[idx]:\
        matchCount +=\cf5 1\cf0 \
        \
\pard\pardeftab720\partightenfactor0
\cf3 # accuracy - get the accuracy of the Naive \ul Bayes\ulnone  Classifier \cf6 # 0.884\cf0 \
\pard\pardeftab720\partightenfactor0
\cf2 print\cf0 (\cf4 "\\\ul tlen\ulnone (labels_test) is \{\}"\cf0 .format(len(labels_test))) \cf3 # \cf0 \
\cf2 print\cf0 (\cf4 "\\tNaive \ul Bayes\ulnone  matchCount is \{\}"\cf0 .format(matchCount)) \cf3 # \cf0 \
\cf2 print\cf0 (\cf4 "\\tNaive \ul Bayes\ulnone  Terrain Classifier Accuracy - matchCount / \ul len\ulnone (labels_test) is \{\}"\cf0 .format(matchCount / len(labels_test))) \cf6 # 0.884\cf3 \
\
# sklearn.metrics.accuracy_score(\ul pred\ulnone ,labels_test), accuracy method used by instructor in video\
\cf0 # Naive Bayes Gaussian Terrain Classifier - ACCURACY 3 of 3 \cf6 # 0.884\cf3 \
\cf0 myGaussianNBTerrainClassifierAAccuracy = sklearn.metrics.\cf6 accuracy_score\cf0 (pred,labels_test)\
\
\
--------------------------------------------------------------------------\
\
** \cf2 Create, instantiate Support Vector Machines - SVM - Terrain Classifier\cf0  **\
\cf2 import\cf0  sklearn.svm\cf3  \
\cf0 ** (\cf2 kernel = 'linear'\cf0 )\cf6  instructor\cf0  **\
SVMclf = sklearn.svm.SVC(kernel = \cf4 'linear'\cf0 ) \
\
# Train - Support Vector Machines - SVM - Terrain Classifier ** learns patterns ** \
SVMclf.fit(features_train, labels_train) ** \cf2 fit - training features, training labels used\cf0  **\
\
** \cf2 predict used in accuracy testing method 2 of 3, and 3 of 3 below \cf0 ** \
# create \cf6 predict\cf0  - Support Vector Machines - SVM - Terrain Classifier\
SVMpred = SVMclf.predict(features_test)\
\
# Support Vector Machines - SVM - Terrain Classifier - ACCURACY 1 of 3 \cf6 # 0.92\cf0 \
my_SVM_clfScore = SVMclf.score(features_test,labels_test)\
\
# Support Vector Machines - SVM - Terrain Classifier - ACCURACY 2 of 3 \cf6 # 0.92\cf0 \
SupportVectorMachinesMatchCount = \cf5 0\cf0 \
\cf2 for\cf0  idx, label \cf2 in\cf0  enumerate(labels_test):\
    \cf3 # print("\\\ul tidx\ulnone  is \{\}".format(\ul idx\ulnone )) # 0 - 249\cf0 \
    \cf3 # print("\\\ul ttype\ulnone (\ul idx\ulnone ) - \{\}\\n".format(type(\ul idx\ulnone ))) # \ul int\ulnone , \ul int\cf0 \ulnone \
    \cf3 # print("\\\ul tlabel\ulnone  is \{\}".format(label)) # 0, 1, 1.0\cf0 \
    \cf3 # print("\\\ul ttype\ulnone (label) - \{\}\\n".format(type(label))) # class '\ul int\ulnone ', class 'float'\cf0 \
    \cf3 # print("\\\ul tpred\ulnone [\ul idx\ulnone ] is \{\}".format(\ul pred\ulnone [\ul idx\ulnone ])) # 0.0, 1.0 ...\cf0 \
    \cf3 # print("\\\ul ttype\ulnone (\ul pred\ulnone [\ul idx\ulnone ]) - \{\}\\n".format(type(\ul pred\ulnone [\ul idx\ulnone ]))) # numpy.float64\cf0 \
    \cf2 if\cf0  label == SVMpred[idx]:\
        SupportVectorMachinesMatchCount +=\cf5 1\cf0 \
        \
\pard\pardeftab720\partightenfactor0
\cf3 # accuracy - get the accuracy of the Support Vector Machines - SVM - Terrain Classifier\cf6  # 0.92\cf0 \
\pard\pardeftab720\partightenfactor0
\cf2 print\cf0 (\cf4 "\\\ul tlen\ulnone (labels_test) is \{\}"\cf0 .format(len(labels_test))) \cf3 # \cf0 \
\cf2 print\cf0 (\cf4 "\\tSupportVectorMachinesMatchCount is \{\}"\cf0 .format(SupportVectorMachinesMatchCount)) \cf3 # \cf0 \
\cf2 print\cf0 (\cf4 "\\tSupport Vector Machine Terrain Classifier Accuracy - SupportVectorMachinesMatchCount / \ul len\ulnone (labels_test) is \{\}\\n"\cf0 .format(SupportVectorMachinesMatchCount / len(labels_test))) \cf6 # 0.92\cf0 \
\
\pard\pardeftab720\partightenfactor0
\cf3 # sklearn.metrics.accuracy_score(\ul pred\ulnone ,labels_test), accuracy method used by instructor in video\cf0 \
# Support Vector Machines - SVM - Terrain Classifier - ACCURACY 3 of 3 \cf6 # 0.92\cf0 \
SupportVectorMachinesSVMTerraiClassifieAccuracy = sklearn.metrics.\cf6 accuracy_score\cf0 (SVMpred,labels_test) \cf6 # 0.92\cf0 \
\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardeftab720\pardirnatural\partightenfactor0

\f0\fs24 \cf0 ***\
*** Introduction to Machine Learning - \cf3 SVM\cf0  - \cf2 Non Linear SVM Support Vector Machines \cf0 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24  of 37 (many lessons) -  5 March 2017 \
***\
\
\pard\pardeftab720\partightenfactor0

\fs28 \cf8 \expnd0\expndtw0\kerning0
There are several different types, kinds of Support Vector Machines (SVM) within scikit-learn.\
The one we use - Support Vector Classifier (SVC).\
Support Vector Classifier (SVC) is one example of a Several different Support Vector Machines (SVM)\
\
\
\pard\pardeftab720\sl280\partightenfactor0
\cf8 The 'gamma' parameter actually has no effect on the 'linear' kernel for SVMs.\
The key parameter for this kernel function is "C", which is described on the following video.\
C parameter example, gamma parameter example\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardeftab720\pardirnatural\partightenfactor0
\cf8 *** Introduction to Machine Learning - \cf3 Support Vector Machines SVM\cf8  - \cf2 Kernel and Gamma\cf8  - 21 of 37 6 March 2017\
\pard\pardeftab720\sl280\partightenfactor0
\cf8 Random Support Vector Classifier (SVC) copied from the online documentation.\
\pard\pardeftab720\sl280\partightenfactor0
\cf2 C and gamma parameter example\cf8  ** \cf2 C parameter, C argument\cf8  **\
Here the Support Vector Classifier (SVC) is being created, before \cf2 .fit\cf8  fitting or training the Classifier \
\pard\pardeftab720\partightenfactor0

\f1\fs26 \cf9 \cb10 SVC(\cf6 C=1.0\cf9 , cache_size=200, class_weight=None, coef0=0.0,\cf11 \
\cf9     decision_function_shape=None, degree=3, \cf6 gamma='auto'\cf9 , \cf6 kernel='rbf'\cf9 ,\cf11 \
\cf9     max_iter=-1, probability=False, random_state=None, shrinking=True,\cf11 \
\cf9     tol=0.001, verbose=False)\
\
\pard\pardeftab720\partightenfactor0
\cf0 Also used - \cf6 kernel='linear'\cf0  - 
\f0\fs28 \cb1 T\cf8 he \cf6 'gamma' parameter\cf8  actually has no effect on the \cf6 'linear' kernel\cf8  for SVMs.
\f1\fs26 \cf6 \cb10 \
\cf0 Instructor used both - \cf6 kernel='linear', kernel='rbf' \
\
\pard\pardeftab720\partightenfactor0

\f0\fs28 \cf8 C is the parameter for the soft margin \cf6 cost\cf8  function\
\pard\pardeftab720\partightenfactor0

\b \cf9 \cb1 Small C makes the cost of misclassificaiton low 
\b0 ("soft margin"), thus allowing more of them for the sake of wider "cushion".\

\b Large C makes the cost of misclassification high 
\b0 ('hard margin"), thus forcing the algorithm to explain the input data stricter and potentially overfit.\
SupportVectorMachines/src/studentMain.py *\cf2 * manipulate, experiment with C parameter, C=\cf9  **\
SupportVectorMachines/src/ClassifyNB.py *\cf2 * manipulate, experiment with C parameter, C=\cf9  **\
\pard\pardeftab720\partightenfactor0
\cf6 I actually coded it - smaller C straighter line, \cf2 C parameter effects on overplotting - \cf6 smaller C straighter line, instructor agrees\
\pard\pardeftab720\partightenfactor0

\f1\fs22 \cf6 \kerning1\expnd0\expndtw0 SVMclf = sklearn.svm.SVC(C=10) - \cf0 ** \cf6 \ul significantly straighter line\cf0 \ulnone  **\cf6 \
SVMclf = sklearn.svm.SVC(C=10000) \cf0 ** \cf6 \ul more training points correct -\cf2  ** instructor **\cf0 \ulnone  **\cf6  LESS STRAIGHT LINE\
C parameter had no effect - my code - when kernel = linear - \cf0 SVMclf = sklearn.svm.SVC(\cf6 kernel = 'linear'\cf0 )\cf2  I actually tried it.\
\
\pard\pardeftab720\partightenfactor0
\cf0 kernel : string, optional (\cf6 default=\'92\ul rbf\ulnone \'92\cf0 ) - \cf2 from documentation\cf0  - 
\f0\fs28 \cf8 \expnd0\expndtw0\kerning0
Support Vector Classifier (SVC) \cf6 default kernel rbf\cf8  radial basis function kernel\cf9 \
\
Gamma is the free parameter of the Gaussian radial basis function.\
\pard\pardeftab720\partightenfactor0
\cf12 Intuitively, the 
\f2\fs26 \cf11 \cb13 gamma
\f0\fs28 \cf12 \cb1  parameter defines how far the influence of a single training example reaches, with low values meaning \'91far\'92 and high values meaning \'91close\'92. \
gamma=0.1 - random example \
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardeftab720\pardirnatural\partightenfactor0
\cf8 rbf - radial basis function kernel  \
\
overfitting - classifier line tries to \cf6 correctly\cf8  classify every point, 100% of the points correctly 100% of the time - \cf6 curvy decision boundary - hyperplane \cf8 lines (taking data too literally)\
\
\cf2 Now code gamma, gamma effects on overplaying\
\pard\pardeftab720\partightenfactor0

\f1\fs22 \cf0 \kerning1\expnd0\expndtw0 SVMclf = sklearn.svm.SVC(kernel=\cf4 '\ul rbf\ulnone '\cf0 , gamma=\cf5 1000\cf0 ) - curvy line - overfitting \
SVMclf = sklearn.svm.SVC(kernel=\cf4 '\ul rbf\ulnone '\cf0 , gamma=\cf5 10\cf0 ) - curvy line - overfitting \
SVMclf = sklearn.svm.SVC(kernel=\cf4 '\ul rbf\ulnone '\cf0 , gamma=\cf5 1\cf0 ) - straighter line \
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardeftab720\pardirnatural\partightenfactor0

\f0\fs28 \cf2 \expnd0\expndtw0\kerning0
Now code kernel, kernel effects on overplotting
\f1\fs22 \cf0 \kerning1\expnd0\expndtw0 \
\pard\pardeftab720\partightenfactor0
\cf0 SVMclf = sklearn.svm.SVC(\cf6 kernel = 'linear'\cf0 ) - I saw straight lines\
SVMclf = sklearn.svm.SVC(\cf6 kernel='\ul rbf\ulnone '\cf0 , gamma=\cf5 1\cf0 ) - I saw curvy lines - example SVMclf = sklearn.svm.SVC(kernel=\cf4 '\ul rbf\ulnone '\cf0 , gamma=\cf5 1000\cf0 )\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardeftab720\pardirnatural\partightenfactor0

\f0\fs24 \cf0 ***\
*** 
\fs28 \cf8 \expnd0\expndtw0\kerning0
Introduction to Machine Learning
\fs24 \cf0 \kerning1\expnd0\expndtw0  - \cf3 SVM\cf0  - 
\fs28 \cf8 \expnd0\expndtw0\kerning0
SVM Author ID Accuracy
\fs24 \cf2 \kerning1\expnd0\expndtw0  27, 28, 29\cf0  of 37 -  6 March 2017 \
***\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardeftab720\pardirnatural\partightenfactor0

\fs28 \cf8 \expnd0\expndtw0\kerning0
Introduction To Machine Learning - SVM - Kernel Trick 19 of 37\
\pard\pardeftab720\sl280\partightenfactor0
\cf8 Import, create, train and make predictions with the sklearn SVC classifier.\
When creating the classifier, use a linear kernel (if you forget this step, you will be unpleasantly surprised by how long the classifier takes to train). What is the accuracy of the classifier?\
\
\pard\pardeftab720\partightenfactor0

\f1\fs22 \cf3 \kerning1\expnd0\expndtw0 # This time using Support Vector Machines (SVM) - Support Vector Classifier (SVC)\cf0 \
\cf3 # Last Time Used Naive \ul Bayes\ulnone  Classifier, Naive \ul Bayes\ulnone  Classifier ran much faster\cf0 \
\cf3 # Last time - Introduction To Machine Learning - Naive \ul Bayes\ulnone  - Machine Learning For Author ID - 40 of 43\cf0 \
\cf3 # Naive \ul Bayes\ulnone  took seconds, Support Vector Machines (SVM) - Support Vector Classifier (SVC) took minutes\cf0 \
\cf3 # Naive \ul Bayes\ulnone  Accuracy - 0.973\cf0 \
\cf3 # Support Vector Machines (SVM) - Support Vector Classifier (SVC) - accuracy - \cf0 \
\cf3 # mySupportVectorMachineClassifierScore - 0.9840728100113766 using \cf0 sklearn.svm.SVC(kernel = \cf4 'linear'\cf0 )\
\
\cf3 # mySupportVectorMachineClassifier = sklearn.svm.SVC() # instructor says - "takes much longer" my timing - about 15 minutes \cf0 \
\cf3 # mySupportVectorMachineClassifier - training time - .fit time - 915.65\
\pard\pardeftab720\partightenfactor0
\cf0 ** Naive Bayes Timing - Support Vector Machines (SVM) - Support Vector Classifier (SVC) Timing Comparison **\
Naive Bayes - training time: - 1.217\
Naive Bayes - predict time: - 0.171\
\
Support Vector Machines (SVM) - Support Vector Classifier (SVC)\
mySupportVectorMachineClassifier - training time - .fit time - 145.971\
mySupportVectorMachineClassifier - predict time - 14.905\
\
** \cf6 instructor\cf0  conclusion - Support Vector Machines (SVM) - Support Vector Classifier (SVC) \cf6 SLOWER\cf0  than Naive Bayes  **\
\
** how to use \cf6 less data\cf0  when training the Classifier, \cf2 improve performance, less time\cf0  ** ** sklearn.svm.SVC(\cf6 kernel = 'linear'\cf0 ) **\
# \cf6 only use 1%\cf0  of \cf2 features_train\cf0 , \cf2 labels_train\cf0  - \cf2 performance increases\cf0 , \cf2 time decreases\cf0 , \cf6 accuracy decreases\cf0 \
features_train = features_train[:round(len(features_train)/\cf5 100\cf0 )] \
labels_train = labels_train[:round(len(labels_train)/\cf5 100\cf0 )]\
mySupportVectorMachineClassifier = sklearn.svm.SVC(kernel = \cf4 'linear'\cf0 ) \
\pard\pardeftab720\partightenfactor0
\cf3 #      mySupportVectorMachineClassifierScore - 0.9840728100113766 # using 100% of features_train and labels_train \cf0 ** ACCURACY / Performance **\
** \cf6 instructor\cf0  says we used 1% of features_train, labels_train - \cf2 accuracy 0.8845278725824801\cf0  - \cf6 instructor says not bad\cf0  **  ** sklearn.svm.SVC(\cf6 kernel = 'linear'\cf0 ) ** \cf2 Classifier / kernel used\cf0  **\
\cf3 #      mySupportVectorMachineClassifierScore - 0.8845278725824801 # accuracy dropped when \ul using\ulnone  1% of features_train and labels_train \cf0 ** ACCURACY / Performance **\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardeftab720\pardirnatural\partightenfactor0

\f0\fs24 \cf0 ***\
*** 
\fs28 \cf8 \expnd0\expndtw0\kerning0
Introduction to Machine Learning
\fs24 \cf0 \kerning1\expnd0\expndtw0  - \cf3 SVM\cf0  - 
\fs28 \cf8 \expnd0\expndtw0\kerning0
Deploy an RBF Kernel 
\fs24 \cf2 \kerning1\expnd0\expndtw0  31\cf0  of 37 -  7 March 2017 \
***
\f1\fs22 \cf6 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardeftab720\pardirnatural\partightenfactor0

\f0\fs28 \cf8 \expnd0\expndtw0\kerning0
mySupportVectorMachineClassifier = sklearn.svm.SVC(kernel = '\cf6 rbf\cf8 ')  ** \cf2 radial basis function kernel\cf8  **\
\pard\pardeftab720\partightenfactor0
\cf8 ** accuracy dropped - mySupportVectorMachineClassifierScore - \cf6 0.61\cf8 60409556313993 ** accuracy - sklearn.svm.SVC(kernel = 'rbf') - \cf6 0.61\cf8 60409556313993\
** \cf6 instructor\cf8  "\cf6 not great accuracy\cf8 "\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardeftab720\pardirnatural\partightenfactor0
\cf8 \
** Introduction to Machine Learning - SVM - \cf2 Optimize C Parameter\cf8  32 of 37 ** -  7 March 2017\
** relationships between C parameter, C argument, Accuracy **\
\
\pard\pardeftab720\partightenfactor0
\cf8 mySupportVectorMachineClassifier = \cf6 sklearn.svm.SVC(kernel = 'rbf')\cf8 \
mySupportVectorMachineClassifierScore - \cf6 0.6160409556313993 \cf8 ACCURACY\
\
1% of the data
\f1\fs22 \cf0 \kerning1\expnd0\expndtw0 \
features_train = features_train[:round(len(features_train)/\cf5 100\cf0 )] \
labels_train = labels_train[:round(len(labels_train)/\cf5 100\cf0 )]
\f0\fs28 \cf8 \expnd0\expndtw0\kerning0
\
mySupportVectorMachineClassifier = \cf6 sklearn.svm.SVC(C=10000,kernel = 'rbf')\cf8   higher C, C= C Parameter, C argument, higher accuracy\
mySupportVectorMachineClassifierScore - \cf6 0.8924914675767918 \cf8 ACCURACY\
\
mySupportVectorMachineClassifier = sklearn.svm.SVC(\cf6 C=1000\cf8 ,kernel = 'rbf') \
mySupportVectorMachineClassifierScore - \cf6 0.8213879408418657\cf8  ACCURACY\
\
mySupportVectorMachineClassifier = sklearn.svm.SVC(\cf6 C=100\cf8 ,kernel = 'rbf') \
mySupportVectorMachineClassifierScore - \cf6 0.6160409556313993\cf8  ACCURACY\
\
mySupportVectorMachineClassifier = sklearn.svm.SVC(\cf6 C=10\cf8 ,kernel = 'rbf') \
mySupportVectorMachineClassifierScore - \cf6 0.6160409556313993\cf8  ACCURACY\
\
features_train, features_test, labels_train, labels_test = email_preprocess.preprocess() ** \cf2 full data set\cf8 , increases ACCURACY**\
mySupportVectorMachineClassifier = sklearn.svm.SVC(C=10000,kernel = 'rbf')  ** \cf2 high C, C=, C Parameter, C argument, \cf0 increased ACCURACY\cf8  **\
mySupportVectorMachineClassifierScore - 0.9908987485779295 ACCURACY \
\
**\cf2  summary\cf8  **  C, C= C Parameter, C argument higher number more accurate, more curvy line, less straight line, **\cf2  summary\cf8  ** \
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardeftab720\pardirnatural\partightenfactor0

\fs24 \cf0 \kerning1\expnd0\expndtw0 ***\
*** 
\fs28 \cf8 \expnd0\expndtw0\kerning0
Introduction to Machine Learning
\fs24 \cf0 \kerning1\expnd0\expndtw0  - \cf3 Decision Trees\cf0  - 41 - 8 March 2017 \
*** 
\fs28 \cf8 \expnd0\expndtw0\kerning0
Introduction to Machine Learning
\fs24 \cf0 \kerning1\expnd0\expndtw0  - \cf3 Decision Trees\cf0  - Introduction 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12 of 41 - 8 March 2017 \
***\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardeftab720\pardirnatural\partightenfactor0
\cf0 \ul Examples of Supervised Classification Algorithms\ulnone \
1.) Naive Bayes\
2.) Supervised Vector Machines - SVM
\fs28 \cf8 \expnd0\expndtw0\kerning0
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardeftab720\pardirnatural\partightenfactor0

\fs24 \cf0 \kerning1\expnd0\expndtw0 3.) Decision Trees \
\
Recap - \
Supervised Vector Machines - SVM used Kernel Trick\
	SVMclf = sklearn.svm.SVC(kernel = 'linear')\
	mySupportVectorMachineClassifier = sklearn.svm.SVC(kernel = 'rbf')\
\
Linear Decision Surface - like a Decision Boundary\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardeftab720\pardirnatural\partightenfactor0
\cf0 \ul Decision Tree Example\ulnone  \
			windy\
	\cf2 yes\cf0 					\cf6 no\cf0  (\cf2 stops right here not going windsurfing, not windy\cf0 ) \
			sunny (\cf2 if it is windy, yes, check for sunny\cf0 )\
yes (\cf2 going windsurfing\cf0 ) 			\cf6 no\cf0  \
\
\pard\pardeftab720\partightenfactor0

\f1\fs22 \cf0 classifier.fit(features_train,labels_train) ** \cf2 fit - training features, training labels used\cf0  **
\f0\fs24 \

\f1\fs22 train or fit the sklearn.tree.tree.DecisionTreeClassifier using training data\
\
\pard\pardeftab720\partightenfactor0

\fs24 \cf0 ** \cf6 In general terms we look at the classifier parameters, arguments to INCREASE accuracy\cf0  **
\fs22 \
\pard\pardeftab720\partightenfactor0
\cf6 a note in the video - min_impurity_split=50\cf0 \
we used - \cf6 min_samples_split = n\
\pard\pardeftab720\partightenfactor0
\cf0 both are probably important, useful\cf3 \ul \ulc3 \
\
\pard\pardeftab720\partightenfactor0
\cf6 \ulnone Relationship between accuracy and classifier instantiation with the min_samples_split parameter \
\pard\pardeftab720\partightenfactor0
\cf0     clf = tree.DecisionTreeClassifier(min_samples_split=\cf6 2\cf0 )\
    \cf3 # accuracy - \cf6 0.912\cf3  - my data from local Eclipse IDE\cf0 \
    \cf3 # accuracy - \cf6 0.908\cf3  - online udacity.com decision Tree Accuracy Quiz - probably different data \cf0 \
    \
    clf = tree.DecisionTreeClassifier(min_samples_split=\cf6 50\cf0 )\
    \cf3 # accuracy - \cf6 0.912\
\cf0     \cf3 # summary as min_samples_split increases so does accuracy\cf0  
\f0\fs24 \
\pard\pardeftab720\partightenfactor0

\fs28 \cf8 \expnd0\expndtw0\kerning0
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardeftab720\pardirnatural\partightenfactor0

\fs24 \cf0 \kerning1\expnd0\expndtw0 ***\
*** 
\fs28 \cf8 \expnd0\expndtw0\kerning0
Introduction to Machine Learning
\fs24 \cf0 \kerning1\expnd0\expndtw0  - \cf3 Decision Trees\cf0  - 41 - 8 March 2017 \
*** 
\fs28 \cf8 \expnd0\expndtw0\kerning0
Introduction to Machine Learning
\fs24 \cf0 \kerning1\expnd0\expndtw0  - \cf3 Decision Trees\cf0  - Data Impurity and Entropy 13, 14, 15, 16, 17, 18, 19, 20, 21 of 41 - 8 March 2017 \
***\
Decision Tree Parameters	- 10 of 41\
Entropy Calculation Part 5	- 20 of 41\
Information Gain - 21 of 41\

\fs28 \cf8 \expnd0\expndtw0\kerning0
  
\fs24 \cf0 \kerning1\expnd0\expndtw0 \
\pard\pardeftab720\partightenfactor0
\cf0 Entropy - measure of impurity in a bunch of examples, entropy is the opposite of purity\
\pard\pardeftab720\partightenfactor0

\fs36 \cf0 one extreme all examples same class - entropy = \cf6 0 
\fs24 \cf0 ** verified **\cf6 \
\pard\pardeftab720\partightenfactor0
\cf0 examples evenly split between all available classes - entropy = \cf6 1 \cf0 ** verified **\cf6 \
\pard\pardeftab720\partightenfactor0

\fs28 \cf8 \expnd0\expndtw0\kerning0
largest value for entropy is 1.0 (maximally impure, examples evenly split between the two class labels, example 2 \cf2 SLOW\cf8 , 2 \cf2 FAST\cf8  below.)\
\pard\pardeftab720\partightenfactor0

\fs24 \cf0 \kerning1\expnd0\expndtw0 \
Controls how a Decision Tree decides where to split the data\
all examples of the same class - entropy = \cf6 0\cf0  - 
\i\fs28 \cf6 \expnd0\expndtw0\kerning0
all observations are of the same class - Thrun!
\i0\fs24 \cf0 \kerning1\expnd0\expndtw0 \
examples evenly split between all classes - entropy = \cf6 1.0\cf2  mathematically maximal value\cf0 , 
\fs28 \cf2 \expnd0\expndtw0\kerning0
 maximally impure state\cf8 , \cf2 maximally impure sample
\fs24 \cf0 \kerning1\expnd0\expndtw0 \
\
terrain bumpiness, and terrain grade, are examples of \cf2 features\
\pard\pardeftab720\partightenfactor0
\cf6 contamination\cf0  -\cf2  points\cf0  in the "\cf2 wrong\cf0 " box, class, classification\
\
\pard\pardeftab720\sl280\partightenfactor0

\fs28 \cf8 \expnd0\expndtw0\kerning0
the log base 2 formula that we use will have a maximal value of 1. (\cf2 video quote)\cf8 \
\pard\pardeftab720\partightenfactor0

\fs24 \cf0 \kerning1\expnd0\expndtw0 \
\pard\pardeftab720\partightenfactor0

\fs28 \cf8 \expnd0\expndtw0\kerning0
Entropy Formula
\fs24 \cf0 \kerning1\expnd0\expndtw0  
\f4\fs36 \
\pard\pardeftab720\sl280\partightenfactor0

\f0\fs28 \cf8 \expnd0\expndtw0\kerning0
** \cf2 lower entropy points toward more organized data, and that a decision tree uses that as a way how to classify events\cf8  **\
\pard\pardeftab720\partightenfactor0

\fs24 \cf0 \kerning1\expnd0\expndtw0 \
\pard\pardeftab720\partightenfactor0
\cf6 minus sign prefix big deal\cf0 \
\pard\pardeftab720\partightenfactor0

\f4\fs48 \cf6 -\cf0 \uc0\u931  \cf6 -\cf0 P
\f5\i\fs36 i
\f4\i0  
\fs48 log
\fs36 2 
\fs48 (p
\f5\i\fs36 i
\f4\i0\fs48 )
\f0\fs28 \cf8 \expnd0\expndtw0\kerning0
 - negative sign is correct per the instructor in the video
\f4\fs48 \cf0 \kerning1\expnd0\expndtw0 \

\f5\i\fs36 i
\f4\i0 \
\pard\pardeftab720\partightenfactor0

\f1\fs22 \cf2 \
\pard\pardeftab720\partightenfactor0

\f4\fs48 \cf0 (p
\f5\i\fs36 i
\f4\i0\fs48 )
\fs36  - 
\f0\fs28 \cf8 \expnd0\expndtw0\kerning0
fraction of examples in class
\f4\fs36 \cf0 \kerning1\expnd0\expndtw0  
\f5\i i \
\pard\pardeftab720\partightenfactor0

\f0\i0\fs28 \cf8 \expnd0\expndtw0\kerning0
\
10 March 2017\
How to calculate
\f4\fs48 \cf0 \kerning1\expnd0\expndtw0 (p
\f5\i\fs36 i
\f4\i0\fs48 )
\f0\fs28 \cf8 \expnd0\expndtw0\kerning0
 an example \
\
\pard\pardeftab720\partightenfactor0

\fs36 \cf8 \ul Parent Node Entropy Calculation Example
\fs28 \
\pard\pardeftab720\partightenfactor0
\cf8 \ulnone Parent Node - 1 \cf6 Node\cf8  - 4 \cf2 Entries\cf8  \cf2 SLOW, SLOW, FAST, FAST \cf0 ** top of decision tree **\cf2 \
\cf8 Parent Node Entropy 1.0\
Remember - examples evenly split between all available classes - entropy = \cf6 1\cf8  ** verified ** \
2 SLOW, 2 FAST, 
\f4\fs48 \cf0 \kerning1\expnd0\expndtw0 p
\f5\i\fs36 i - 
\f0\i0\fs28 \cf8 \expnd0\expndtw0\kerning0
SLOW
\f5\i\fs36 \cf0 \kerning1\expnd0\expndtw0  - 
\f0\i0\fs28 \cf8 \expnd0\expndtw0\kerning0
0.50, 
\f4\fs48 \cf0 \kerning1\expnd0\expndtw0 p
\f5\i\fs36 i - 
\f0\i0\fs28 \cf8 \expnd0\expndtw0\kerning0
FAST
\f5\i\fs36 \cf0 \kerning1\expnd0\expndtw0  - 
\f0\i0\fs28 \cf8 \expnd0\expndtw0\kerning0
0.50
\f5\i\fs36 \cf0 \kerning1\expnd0\expndtw0  
\f0\i0\fs28 \cf8 \expnd0\expndtw0\kerning0
- meets the 
\i \cf6 evenly split
\i0 \cf8  criteria, the two 
\f4\fs48 \cf0 \kerning1\expnd0\expndtw0 p
\f5\i\fs36 i
\f0\i0\fs28 \cf8 \expnd0\expndtw0\kerning0
 are the SAME - \cf6 Entropy\cf8  of the Parent Node - 1.0\
\pard\pardeftab720\partightenfactor0

\f4\fs48 \cf0 \kerning1\expnd0\expndtw0 p
\f5\i\fs36 i - 
\f0\i0\fs28 \cf8 \expnd0\expndtw0\kerning0
SLOW
\f5\i\fs36 \cf0 \kerning1\expnd0\expndtw0  - 
\f0\i0\fs28 \cf8 \expnd0\expndtw0\kerning0
0.50, 
\f4\fs48 \cf0 \kerning1\expnd0\expndtw0 p
\f5\i\fs36 i - 
\f0\i0\fs28 \cf8 \expnd0\expndtw0\kerning0
FAST
\f5\i\fs36 \cf0 \kerning1\expnd0\expndtw0  - 
\f0\i0\fs28 \cf8 \expnd0\expndtw0\kerning0
0.50
\f4\fs36 \cf6 \kerning1\expnd0\expndtw0 \
-
\f0\fs28 \cf0 \expnd0\expndtw0\kerning0
( 0.5(log\sub 2\nosupersub (0.5)) ) 
\f4\fs36 \cf6 \kerning1\expnd0\expndtw0 - 
\f0\fs28 \cf0 \expnd0\expndtw0\kerning0
( 0.5(log\sub 2\nosupersub (0.5)) )
\f4\fs36 \cf6 \kerning1\expnd0\expndtw0  
\f0\fs28 \cf8 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720\partightenfactor0

\f4\fs36 \cf6 \kerning1\expnd0\expndtw0 -
\f0\fs28 \cf0 \expnd0\expndtw0\kerning0
( 0.5(
\f4\fs36 \cf6 \kerning1\expnd0\expndtw0 -
\f0\fs28 \cf0 \expnd0\expndtw0\kerning0
1) ) 
\f4\fs36 \cf6 \kerning1\expnd0\expndtw0 - 
\f0\fs28 \cf0 \expnd0\expndtw0\kerning0
( 0.5(
\f4\fs36 \cf6 \kerning1\expnd0\expndtw0 -
\f0\fs28 \cf0 \expnd0\expndtw0\kerning0
1) ) - ** \cf2 watch the minus signs\cf0  **\
 0.5 
\f4\fs36 \cf6 \kerning1\expnd0\expndtw0 - 
\f0\fs28 \cf0 \expnd0\expndtw0\kerning0
( 0.5(
\f4\fs36 \cf6 \kerning1\expnd0\expndtw0 -
\f0\fs28 \cf0 \expnd0\expndtw0\kerning0
1) )\
 0.5 
\f4\fs36 \cf6 \kerning1\expnd0\expndtw0 - 
\f0\fs28 \cf0 \expnd0\expndtw0\kerning0
(
\f4\fs36 \cf6 \kerning1\expnd0\expndtw0 -
\f0\fs28 \cf0 \expnd0\expndtw0\kerning0
0.5)\
 0.5 
\f4\fs36 \cf6 \kerning1\expnd0\expndtw0 + 
\f0\fs28 \cf0 \expnd0\expndtw0\kerning0
0.5\
1.0 - Parent Node Entropy 1.0 (maximally impure state, examples evenly split, total 4, 2 SLOW, 2 FAST)\

\f4\fs36 \cf6 \kerning1\expnd0\expndtw0  
\f0\fs28 \cf8 \expnd0\expndtw0\kerning0
\
\
Grade Nodes - \cf6 2\cf8  Nodes, \cf2 STEEP, FLAT\
\pard\pardeftab720\partightenfactor0
\cf0 4 \cf6 Entries\cf0  (
\i all 4 entries
\i0 ) \cf6 inherited\cf0  from Parent Node - \cf2 SLOW, SLOW, FAST, FAST\cf0  \cf2 \
\cf6 STEEP\cf2  Node - \cf0 3 Entries\cf2  SLOW, SLOW, FAST\cf0  - 3 of the 4 are here  
\f4\fs48 \kerning1\expnd0\expndtw0 (p
\f5\i\fs36 i
\f4\i0\fs48 )
\f0\fs28 \expnd0\expndtw0\kerning0
 = 3/4\
SLOW -> log\sub 2\nosupersub (2/3) -> log\sub 2\nosupersub (0.6666) 
\f4\fs36 \cf6 \kerning1\expnd0\expndtw0 -0.585107 
\f0\fs28 \cf0 \expnd0\expndtw0\kerning0
 \
FAST -> log\sub 2\nosupersub (1/3) -> log\sub 2\nosupersub (0.333)  
\f4\fs36 \cf6 \kerning1\expnd0\expndtw0 -1.585107\

\f0\fs28 \cf0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720\partightenfactor0
\cf0 \ul Entropy Calculation\ulnone  for the \cf6 \ul \ulc6 STEEP\cf0 \ulnone  Node (bass only on the \cf6 3\cf0  entries here, \cf2 SLOW, SLOW, FAST\cf0 )\
Entropy - STEEP Node - 2 SLOW (2/3), 1 FAST (1/3)\
                 \cf2 2 SLOW (2/3)                       1 FAST (1/3)\cf0 \
 
\f4\fs36 \cf6 \kerning1\expnd0\expndtw0 -(0.666(-0.585107)) - (0.333(-1.585107)) xxx\
\pard\pardeftab720\partightenfactor0

\f0\fs28 \cf0 \expnd0\expndtw0\kerning0
 
\f4\fs36 \cf6 \kerning1\expnd0\expndtw0 -(0.666(-0.585107)) - (0.333(-1.585107))\
		   -(-0.3897) - (-0.5278)\
		   -(-0.3897) + 0.5278\
		     + 0.3897 + 0.5278\
\pard\pardeftab720\partightenfactor0
\cf2 			0.9175 - Entropy Calculation - STEEP Node, \cf0 (child node)\cf2 , SLOW, SLOW, FAST ( 
\f0\i\fs28 \cf6 \expnd0\expndtw0\kerning0
video - instructor agrees, right answer - video says 0.9184
\f4\i0\fs36 \kerning1\expnd0\expndtw0  \cf2 )\
\
\pard\pardeftab720\partightenfactor0

\fs28 \cf0 Weighted Average needed for Information gain below
\fs36 \cf2 \

\fs28 \cf0 Information Gain \cf6 Weighted\cf0  Average "\cf2 Grade (STEEP, FLAT)\cf0 " Children Entropy (4 total children) both the STEEP Node and the FLAT Node\
STEEP Node - 3 of the 4 children here (3/4)\
\pard\pardeftab720\partightenfactor0
\cf0 \ul percent of entries in that 1 node times the entropy for that 1 node\
\pard\pardeftab720\partightenfactor0
\cf0 \ulnone 0.75 * 0.9175 = 0.688125
\fs36 \
\pard\pardeftab720\partightenfactor0

\f0\fs28 \cf8 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720\partightenfactor0
\cf0 \ul \ulc0 Entropy Calculation\ulnone  for the \cf6 \ul \ulc6 FLAT\cf0 \ulnone  Node - 1 entry \cf2 FAST\cf0 \
\pard\pardeftab720\partightenfactor0

\fs36 \cf0 \kerning1\expnd0\expndtw0 all examples of the same class - entropy = \cf6 0
\fs24 \cf0  
\fs28 \cf8 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720\partightenfactor0
\cf6 FLAT\cf2     Node - \cf0 1 Entry \cf2 FAST \cf0  - 1 of the 4 are here  
\f4\fs48 \kerning1\expnd0\expndtw0 (p
\f5\i\fs36 i
\f4\i0\fs48 )
\f0\fs28 \expnd0\expndtw0\kerning0
 = 1/4\
Entropy - FLAT Node - 1 FAST (1/1)\
\pard\pardeftab720\partightenfactor0
\cf2 1 FAST (1/1)\cf0 \
\pard\pardeftab720\partightenfactor0

\f4\fs36 \cf6 \kerning1\expnd0\expndtw0 -
\f0\fs28 \cf0 \expnd0\expndtw0\kerning0
( 1(log\sub 2\nosupersub (1/1)) )
\i \cf6 \

\f4\i0\fs36 \kerning1\expnd0\expndtw0 -
\f0\fs28 \cf0 \expnd0\expndtw0\kerning0
( 1(\cf6 0\cf0 ) )
\i \cf6 \

\f4\i0\fs36 \kerning1\expnd0\expndtw0 -
\f0\fs28 \expnd0\expndtw0\kerning0
0
\i \
\pard\pardeftab720\partightenfactor0

\i0 \cf6 0 
\f4\fs36 \cf2 \kerning1\expnd0\expndtw0  - Entropy Calculation - FLAT Node, \cf0 (child node)\cf2 , FAST\
\pard\pardeftab720\partightenfactor0

\fs28 \cf0 Information Gain \cf6 Weighted\cf0  Average "\cf2 Grade\cf0 " Children Entropy (4 total children)\
FAST Node - 1 of the 4 children here (1/1)\
\pard\pardeftab720\partightenfactor0
\cf0 \ul percent of entries in that 1 node times the entropy for that 1 node\ulnone \
1 * 0 = 0
\fs36 \
\
\pard\pardeftab720\partightenfactor0

\fs28 \cf6 Child Information Gain Calculation\cf0  for the 4 Entries, 2 Grade Nodes (\cf2 STEEP, FAST\cf0 )
\fs36 \
\pard\pardeftab720\partightenfactor0

\fs28 \cf0 1 - 0.688125 - 0 = 0.311875
\fs36 \

\fs28  
\f0 \cf8 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720\partightenfactor0

\fs24 \cf0 \kerning1\expnd0\expndtw0 \
\
\
\pard\pardeftab720\partightenfactor0
\cf6 minus sign prefix big deal\cf0 \
\pard\pardeftab720\partightenfactor0

\f4\fs48 \cf6 - \cf0 \uc0\u931 
\f0\fs28 \cf8 \expnd0\expndtw0\kerning0
 - sum over all classes available
\fs24 \cf0 \kerning1\expnd0\expndtw0 \
\pard\pardeftab720\partightenfactor0

\f1\fs22 \cf2 \
\pard\pardeftab720\partightenfactor0

\f0\fs28 \cf8 \expnd0\expndtw0\kerning0
grade		bumpiness		speed		speed\
					limit?\
\pard\pardeftab720\partightenfactor0

\f1\fs22 \cf0 \kerning1\expnd0\expndtw0 ----------------------------------------------------\
steep		bumpy			yes			slow\
steep		smooth		yes			slow\
flat		bumpy			no			fast\
steep		smooth		no			fast\
\
\pard\pardeftab720\partightenfactor0

\f0\fs28 \cf2 \expnd0\expndtw0\kerning0
\ul \ulc2 from the speed column\cf8 \ulnone \
\pard\pardeftab720\partightenfactor0

\f1 \cf0 \kerning1\expnd0\expndtw0 SLOW SLOW FAST FAST (2 '
\f5\i SLOW
\f1\i0 ',2 '
\f5\i FAST
\f1\i0 ', 4 TOTAL)\
\
\pard\pardeftab720\partightenfactor0

\f4\fs48 \cf0 p
\f5\i\fs36 slow - 
\f0\i0\fs28 \cf2 \expnd0\expndtw0\kerning0
fraction of slow examples\cf8  - 2 / 4 = \cf6 0.50\cf8 \

\f4\fs48 \cf0 \kerning1\expnd0\expndtw0 p
\f5\i\fs36 fast - 
\f0\i0\fs28 \cf2 \expnd0\expndtw0\kerning0
fraction of slow examples\cf8  - 2 / 4 = \cf6 0.50\
\
\pard\pardeftab720\partightenfactor0
\cf8 log\sub 2\nosupersub  of 0.5 = -1\
\
entropy formula examples sum sigma "log2"\
\
\pard\pardeftab720\partightenfactor0
\cf2 example from google search\cf8 \
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf8 H=\uc0\u8722 0.4log2(0.4) 
\fs36 \cf6 \uc0\u8722 
\fs28 \cf8  0.6log2(0.6)\uc0\u8776 \u8722 0.4(\u8722 1.3) 
\fs36 \cf6 \uc0\u8722 
\fs28 \cf8  0.6(\uc0\u8722 0.7)=.52+.42=.94 bits\
\
Entropy calculation for parent \cf2 SLOW, SLOW, FAST, FAST\cf8 \
\cf2 minus sign in the middle driven by \cf8 \
\pard\pardeftab720\partightenfactor0

\fs24 \cf6 \kerning1\expnd0\expndtw0 minus sign prefix big deal\cf0 \
\pard\pardeftab720\partightenfactor0

\f4\fs48 \cf6 - \cf0 \uc0\u931 
\f0\fs28 \cf8 \expnd0\expndtw0\kerning0
 - sum over all classes available\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf8 H= \uc0\u8722 0.5 log2(0.5) 
\fs36 \cf6 \uc0\u8722 
\fs28 \cf8  0.5 log2(0.5)\
H= ( \uc0\u8722 0.5 (log2(0.5)) ) 
\fs36 \cf6 \uc0\u8722 
\fs28 \cf8  ( 0.5 (log2(0.5)) )\
H= ( \uc0\u8722 0.5 ( -1) ) 
\fs36 \cf6 \uc0\u8722 
\fs28 \cf8  ( 0.5 (-1) )\
H= ( 0.5) 
\fs36 \cf6 \uc0\u8722 
\fs28 \cf8  ( - 0.5)\
H= ( 0.5) 
\fs36 \cf6 \uc0\u8722 
\fs28 \cf8  (0.5)\
H= 1.0 entropy = 1.0
\fs24 \cf2 \kerning1\expnd0\expndtw0 mathematically maximal value\cf0 , 
\fs28 \cf2 \expnd0\expndtw0\kerning0
 maximally impure state\cf8 , \cf2 maximally impure sample\
examples evenly split between the two class labels, we have 4 total 2 fast, 2 slow - entropy = 1.0\
\
\pard\pardeftab720\partightenfactor0
\cf8 Information Gain\
information gain 
\fs36 \cf6 =
\fs28 \cf8  entropy (parent) 
\fs36 \cf6 \uc0\u8722 
\fs28 \cf8  [weighted average] entropy (children)\
"\cf2 entropy of the parent\cf8 " \cf6 minus the\cf8  "\cf2 weighted average of the entropy of the children\cf8 "\
\
Decison Tree algorithm will maximize information gain\
\
\pard\pardeftab720\partightenfactor0

\fs36 \cf8 \ul \ulc8 Parent Node Entropy Calculation Example
\fs28 \ulnone  (\cf2 above\cf8 )\
\pard\pardeftab720\partightenfactor0
\cf6 *** \cf8 Entropy of Parent = \cf6 1.0 ***\
\pard\pardeftab720\partightenfactor0
\cf2 SLOW, SLOW, FAST, FAST \cf6 *** \cf8 Entropy of Parent = \cf6 1.0 ***\cf2 \
\
\pard\pardeftab720\partightenfactor0
\cf0 \ul \ulc0 	grade steep flat first column\cf2 \ulnone \
HOW MANY			HOW MANY\
STEEP				FLAT\
\pard\pardeftab720\partightenfactor0
\cf6 	3					1\
\cf8 \
\pard\pardeftab720\partightenfactor0
\cf0 \ul 	parent column speed\cf2 \ulnone \
HOW MANY			HOW MANY\
SLOW				FAST\
\pard\pardeftab720\partightenfactor0
\cf6 	2					2\
\
\pard\pardeftab720\partightenfactor0
\cf0 \ul 	grade column\cf2 \ulnone \
HOW MANY			HOW MANY\
STEEP				FLAT\
\pard\pardeftab720\partightenfactor0
\cf6 	2
\i slow ones
\i0 			1
\i fast ones \cf2 <----------- entropy of this node (both \cf6 FAST\cf2  and \cf6 FLAT\cf2  terrain) =\cf6  0 all observations, there is only 1, are of the same class
\i0 \
	1
\i fast ones                                                      \cf2 entropy arrived at by definition, not calculation, all entries, in this case only 1, are in the SAME CLASS\
\pard\pardeftab720\partightenfactor0
\cf6 entropy\cf2  (2 slow, 1 fast node) -\cf6  
\i0 0.9184
\i \cf2                     entropy = 0 by definition - all examples are in the same class\
\pard\pardeftab720\partightenfactor0

\f4\i0\fs36 \cf6 \kerning1\expnd0\expndtw0                                                        -
\f0\fs28 \cf0 \expnd0\expndtw0\kerning0
( 1(log\sub 2\nosupersub (1/1)) )
\i \cf6 \

\f4\i0\fs36 \kerning1\expnd0\expndtw0                                                        -
\f0\fs28 \cf0 \expnd0\expndtw0\kerning0
( 1(\cf6 0\cf0 ) )
\i \cf6 \

\f4\i0\fs36 \kerning1\expnd0\expndtw0                                                        -
\f0\fs28 \expnd0\expndtw0\kerning0
0
\i \

\f4\i0\fs36 \kerning1\expnd0\expndtw0                                                        
\f0\fs28 \expnd0\expndtw0\kerning0
0
\i \
\
\pard\pardeftab720\partightenfactor0
\cf0 \ul Entropy calculations after the first Decision Tree Split\ulnone  (\cf2 SLOW
\i0 \cf8  
\fs36 \cf6 \uc0\u8722 
\fs28 \cf8  
\i \cf2 FAST\cf0 )\cf6 \
\cf0 Entropy calculations for\cf2  SLOW
\i0\fs36 \cf6 \uc0\u8722 
\fs28 \cf8  
\i \cf2 STEEP \cf0 (child left side)\cf6 \
\cf0 left grade \cf2 STEEP\cf6 \
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\i0 \cf8  \cf6 2 / 3\cf0  = \cf2 3 total STEEP\cf0  (2 SLOW ones, 1 FAST), hence 2 of the 3 total are SLOW or \cf6 2 / 3 \
\cf8 \
\pard\pardeftab720\partightenfactor0

\i \cf0 left grade \cf2 STEEP\cf6 \
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardeftab720\pardirnatural\partightenfactor0

\i0 \cf8  \cf6 1 / 3\cf0  = \cf2 3 total STEEP\cf0  (2 SLOW ones, 1 FAST), hence 1 of the 3 total are FAST or \cf6 1 / 3 \
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf8 H=\uc0\u8722 0.4log2(0.4) 
\fs36 \cf6 \uc0\u8722 
\fs28 \cf8  0.6log2(0.6)\uc0\u8776 \u8722 0.4(\u8722 1.3) 
\fs36 \cf6 \uc0\u8722 
\fs28 \cf8  0.6(\uc0\u8722 0.7)=.52+.42=.94 bits - \cf2 google search example formula\cf8 \
\
Entropy calculation for the \cf2 SLOW\cf8  
\fs36 \cf6 \uc0\u8722 
\fs28 \cf8  \cf2 STEEP\cf8  node\
2 class labels \cf2 SLOW, FAST\cf8 \
H=\uc0\u8722 1/3(log2(1/3)) 
\fs36 \cf6 \uc0\u8722 
\fs28 \cf8  2/3(log2(2/3))\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\fs36 \cf6 \uc0\u8722 
\fs28 \cf8 (1/3(
\fs36 \cf6 \uc0\u8722 
\fs28 \cf8 1.585107)) 
\fs36 \cf6 \uc0\u8722 
\fs28 \cf8   2/3(
\fs36 \cf6 \uc0\u8722 
\fs28 \cf8 0.585107)\

\fs36 \cf6 \uc0\u8722 
\fs28 \cf8 (
\fs36 \cf6 \uc0\u8722 
\fs28 \cf8 0.528369)
\fs36 \cf6  \uc0\u8722 
\fs28 \cf8  (
\fs36 \cf6 \uc0\u8722 
\fs28 \cf8 0.3900)\
0.528369
\fs36 \cf6  + 
\fs28 \cf8 0.3900\
0.918369 = Entropy calculation for the \cf2 SLOW\cf8  
\fs36 \cf6 \uc0\u8722 
\fs28 \cf8  \cf2 STEEP\cf8  node\
\
Entropy calculations using Eclipse / Python\
\pard\pardeftab720\partightenfactor0

\f1\fs22 \cf0 \kerning1\expnd0\expndtw0 myEntropy = scipy.stats.entropy([\cf5 2\cf0 ,\cf5 1\cf0 ],base=\cf5 2\cf0 ) \
\pard\pardeftab720\partightenfactor0
\cf2 print\cf0 (\cf7 "myEntropy - \{\}\\n"\cf0 .format(myEntropy))\
myEntropy - 0.9182958340544894\
\
\pard\pardeftab720\partightenfactor0

\f0\fs36 \cf6 \expnd0\expndtw0\kerning0
\ul \ulc6 Review
\fs28 \cf8 \ulc8 \
\pard\pardeftab720\partightenfactor0
\cf8 \ulnone What is the entropy of this node \cf2 SLOW, SLOW, FAST, FAST Entropy calculation for parent node \cf6 SLOW, SLOW FAST, FAST\cf2 \
\
0.5 SLOW, 0.5 FAST\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf8 H=
\fs36 \cf6 \uc0\u8722 
\fs28 \cf8 0.4log2(0.4) 
\fs36 \cf6 \uc0\u8722 
\fs28 \cf8  0.6log2(0.6)\uc0\u8776 \u8722 0.4(\u8722 1.3) 
\fs36 \cf6 \uc0\u8722 
\fs28 \cf8  0.6(\uc0\u8722 0.7)=.52+.42=.94 bits \cf2 example formula from google search\cf8 \
\
SLOW First left side of the  
\fs36 \cf6 \uc0\u8722 
\fs28 \cf8   in the middle\

\fs36 \cf6 \uc0\u8722 
\fs28 \cf8 (0.5(log2(0.5)))
\f1\fs22 \cf0 \kerning1\expnd0\expndtw0 \

\f0\fs36 \cf6 \expnd0\expndtw0\kerning0
\uc0\u8722 
\fs28 \cf8 (0.5(
\fs36 \cf6 \uc0\u8722 
\fs28 \cf8 1))\
0.5\
\
do not use the middle 
\fs36 \cf6 \uc0\u8722 
\fs28 \cf8  here\
Now FAST\
(0.5(log2(0.5)))
\f1\fs22 \cf0 \kerning1\expnd0\expndtw0 \

\f0\fs28 \cf8 \expnd0\expndtw0\kerning0
(0.5(
\fs36 \cf6 \uc0\u8722 
\fs28 \cf8 1))\

\fs36 \cf6 \uc0\u8722 
\fs28 \cf8 0.5\
\
use the middle 
\fs36 \cf6 \uc0\u8722 
\fs28 \cf8  here\
0.5 
\fs36 \cf6 \uc0\u8722 
\fs28 \cf8  (\uc0\u8722 0.5) = 
\f1\fs22 \cf0 \kerning1\expnd0\expndtw0 \

\f0\fs28 \cf8 \expnd0\expndtw0\kerning0
0.5 
\fs36 \cf6 + 
\fs28 \cf8 0.5 = 1 - entropy one\
\
***\
***\
***\
Calculate entropy for the 2 SLOW, 1 FAST NODE\
STEEP NODE = 2 SLOW, 1 FAST\
\
SLOW\

\fs36 \cf6 \uc0\u8722 
\fs28 \cf8 (2/3(log\sub 2\nosupersub (2/3)))\

\fs36 \cf6 \uc0\u8722 
\fs28 \cf8 (2/3( 
\fs36 \cf6 \uc0\u8722 
\fs28 \cf8 0.585107))\

\fs36 \cf6 \uc0\u8722 
\fs28 \cf8 (-0.3901)\
0.3901\
\
FAST\
(1/3(log\sub 2\nosupersub (1/3)))\
1/3(
\fs36 \cf6 \uc0\u8722 
\fs28 \cf8 1.585107)\

\fs36 \cf6 \uc0\u8722 
\fs28 \cf8 0.528369\
\
0.3901 
\fs36 \cf6 \uc0\u8722  
\fs28 \cf8 (
\fs36 \cf6 \uc0\u8722 
\fs28 \cf8 0.528369)\
0.3901 
\fs36 \cf6 + 
\fs28 \cf8 0.528369\
\cf6 0.9184\cf8  - entropy 2 SLOW, 1 FAST NODE\
\
\cf2 \
\pard\pardeftab720\partightenfactor0
\cf8 Information Gain\
Information Gain Calculation Based on Grade \cf2 STEEP, FLAT\cf8 \
Information Gain 
\fs36 \cf6 =
\fs28 \cf8  entropy (parent) 
\fs36 \cf6 \uc0\u8722 
\fs28 \cf8  [weighted average] entropy (children)\
"\cf2 entropy of the parent\cf8 " \cf6 minus the\cf8  "\cf2 weighted average of the entropy of the children\cf8 "\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf8 \
\pard\pardeftab720\partightenfactor0
\cf8 Information Gain 
\fs36 \cf6 =
\fs28 \cf8  entropy (parent) 
\fs36 \cf6 \uc0\u8722 
\fs28 \cf8  [weighted average] entropy (children)\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf8 \
\pard\pardeftab720\partightenfactor0
\cf8 entropy (parent) (\cf6 SLOW, SLOW, FAST, FAST\cf8 ) = \cf6 1.0\cf8  calculations above\
\
next children based on grade - STEEP, FLAT \
\
STEEP NODE \
SLOW	
\fs36 \cf6 \uc0\u8722 
\fs28 \cf8 	2 SLOW and 1 FAST - TOTAL 3\
entropy - 0.9184\
\
FLAT NODE\
FAST 	
\fs36 \cf6 \uc0\u8722 
\fs28 \cf8 	1 FAST - TOTAL 1\
entropy - 0\
\
\
[weighted average] entropy (children)\
Children based on grade TOTAL examples - 4\
\
Parent node \cf2 SLOW, SLOW, FAST, FAST 4 \cf0 Total\cf8 \
3/4 * (0.9184) = 0.6888\
1/4 * 0 = 0\
\
information gain 
\fs36 \cf6 =
\fs28 \cf8   parent entropy 
\fs36 \cf6 \uc0\u8722 
\fs28 \cf8  weighted children average\
information gain 
\fs36 \cf6 =
\fs28 \cf8   1 - 0.6888 = \cf6 0.3112\cf2  right answer\cf8 \
\
*** Information Gain Calculation Part 7 Bumpiness 28 of 41  ** \cf6 DO NOT GO BACK\cf8  **\
\

\fs36 \ul Parent Node Entropy Calculation Example
\fs28 \ulnone  (\cf2 above\cf8 )\
\pard\pardeftab720\partightenfactor0
\cf6 *** \cf8 Entropy of Parent = \cf6 1.0 ***\
\pard\pardeftab720\partightenfactor0
\cf2 SLOW, SLOW, FAST, FAST \cf6 *** \cf8 Entropy of Parent = \cf6 1.0 ***\cf2 \
\
\pard\pardeftab720\partightenfactor0
\cf0 \ul \ulc0 	\ulnone bumpiness (bumpy, smooth)\cf2 \
HOW MANY			HOW MANY\
bumpy				smooth\
\pard\pardeftab720\partightenfactor0
\cf6 2					2\
slow, fast				slow, fast\cf0  - 4 from parent\cf6 \
\cf8 \
slow 1 of 2				slow 1 of 2\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\fs36 \cf6 \uc0\u8722 
\fs28 \cf8 (0.5(log2(0.5)))
\f1\fs22 \cf0 \kerning1\expnd0\expndtw0 			
\f0\fs36 \cf6 \expnd0\expndtw0\kerning0
\uc0\u8722 
\fs28 \cf8 (0.5(log2(0.5)))
\f1\fs22 \cf0 \kerning1\expnd0\expndtw0 \

\f0\fs36 \cf6 \expnd0\expndtw0\kerning0
\uc0\u8722 
\fs28 \cf8 (0.5(
\fs36 \cf6 \uc0\u8722 
\fs28 \cf8 1))				
\fs36 \cf6 \uc0\u8722 
\fs28 \cf8 (0.5(
\fs36 \cf6 \uc0\u8722 
\fs28 \cf8 1))\
0.5						0.5\
\
fast 1 of 2				fast 1 of 2\

\fs36 \cf6 \uc0\u8722 
\fs28 \cf8 (0.5(log2(0.5)))			
\fs36 \cf6 \uc0\u8722 
\fs28 \cf8 (0.5(log2(0.5)))
\f1\fs22 \cf0 \kerning1\expnd0\expndtw0 \

\f0\fs36 \cf6 \expnd0\expndtw0\kerning0
\uc0\u8722 
\fs28 \cf8 (0.5(
\fs36 \cf6 \uc0\u8722 
\fs28 \cf8 1))				
\fs36 \cf6 \uc0\u8722 
\fs28 \cf8 (0.5(
\fs36 \cf6 \uc0\u8722 
\fs28 \cf8 1))\
0.5						0.5\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf2 entropy for this node		entropy for this node\cf8 		\
0.5 + 0.5 = 1				0.5 + 0.5 = 1\
\
\pard\pardeftab720\partightenfactor0
\cf8 information gain 
\fs36 \cf6 =
\fs28 \cf8   parent entropy 
\fs36 \cf6 \uc0\u8722 
\fs28 \cf8  weighted children average\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf8 \
1.0 - 1/2(1) - 1/2(1) = 0 information gain \cf2 correct answer\
\
\cf0 Information Gain Speed Limit\
\
Parent Node \cf2 SSFF\cf0  \
Parent Node Entropy Calculations Above\cf8 \
\cf0 Parent Node Entropy - 1.0\
\
speed limit - YES\
slow, slow\cf8 \
entropy - 0\
\cf0 \
speed limit - No\
fast, fast\cf8 \
entropy - 0\
\
\pard\pardeftab720\partightenfactor0
\cf8 information gain 
\fs36 \cf6 =
\fs28 \cf8   parent entropy 
\fs36 \cf6 \uc0\u8722 
\fs28 \cf8  weighted children average entropy\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf8 1.0 - 1/2(0) - 1/2(0) = 1\
1 - Best Information Gain possible - this is definitely where we want to make the split\
\
\cf6 Entropy\cf8  and \cf6 Information Gain\cf8  as it relates to \cf6 DecisionTreeClassifier,\cf8  \cf6 Decision Tree Classifier\cf8  creation, instantiation  \
\pard\pardeftab720\partightenfactor0

\i \cf11 \cb10 class 
\f2\i0\fs26 sklearn.tree.
\f6\b\fs28 DecisionTreeClassifier
\f0\b0\fs34 (
\i\fs28 \cf6 criterion='gini'
\i0 \cf11 ,\
\pard\pardeftab720\parhyphenfactor20\partightenfactor0
\cf12 \cb1 The function to measure the quality of a split. Supported criteria are \'93gini\'94 for the Gini impurity and \cf6 \'93entropy\'94 for the information gain\cf12 .\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardeftab720\pardirnatural\partightenfactor0

\fs24 \cf0 \kerning1\expnd0\expndtw0 ***\
*** 
\fs28 \cf8 \expnd0\expndtw0\kerning0
Introduction to Machine Learning
\fs24 \cf0 \kerning1\expnd0\expndtw0  - \cf3 Decision Trees\cf0  - Bias-Variance Dilemma 33 of 41 - 11 March 2017 \
***\
\
\cf2 Bias Variance Dilema -	\cf0 \
Machine Learning Term - \cf2 Bias\
High Bias - practically ignores the data, the training, almost no capacity  to learn anything
\fs28 \cf8 \expnd0\expndtw0\kerning0
\

\fs24 \cf0 \kerning1\expnd0\expndtw0 Machine Learning Term - \cf2 Variance - High Variance - Only replicate stuff it has seen before.\
Bias-Variance tradeoff - algorithm with some authority to generalize, still listens to the data \
\
\cf0 ***\
*** 
\fs28 \cf8 \expnd0\expndtw0\kerning0
Introduction to Machine Learning
\fs24 \cf0 \kerning1\expnd0\expndtw0  - \cf3 Decision Trees\cf0  - Decision Trees Strengths and Weaknesses  34 of 41 - 11 March 2017 \
***\
\
Decision Trees Prone to Overfitting\
Sebastian says amazing \
\
***\
*** 
\fs28 \cf8 \expnd0\expndtw0\kerning0
Introduction to Machine Learning
\fs24 \cf0 \kerning1\expnd0\expndtw0  - \cf3 Decision Trees\cf0  - Decision Tree Mini-Project Video 35 of 41 - 11 March 2017 \
*** 
\fs28 \cf8 \expnd0\expndtw0\kerning0
Introduction to Machine Learning
\fs24 \cf0 \kerning1\expnd0\expndtw0  - \cf3 Decision Trees\cf0  - Decision Tree Mini-Project 36 of 41 - 11 March 2017 \
***\
\
How to access the first row of an 
\f1\fs22 numpy.ndarray\
\pard\pardeftab720\partightenfactor0
\cf2 print\cf0 (\cf4 "type(features_train) - \{\}"\cf0 .format(type(features_train)))
\f0\fs24 \

\f1\fs22 type(features_train) - <class '\cf6 numpy.ndarray\cf0 '>\
\
first row\
\cf2 print\cf0 (\cf4 "features_test[0] - \{\}"\cf0 .format(features_test[\cf6 0\cf0 ]))\
features_test[0] - [ 0.  0.  0. ...,  0.  0.  0.]\
\
length first row\
\cf2 print\cf0 (\cf4 "\cf4 \ul \ulc4 len\cf4 \ulnone (features_test[\cf6 0\cf4 ]) - \{\}"\cf0 .format(\cf6 len\cf0 (features_test[\cf6 0\cf0 ])))\
\cf6 len\cf0 (features_test[0]) - \cf6 3785\
\
lower \cf0 percentile=\cf6 1 - ** faster training time ** \
\cf0 selector = SelectPercentile(f_classif, percentile=\cf6 10\cf0 ) ** \cf2 out of box sklearn.tree.DecisionTreeClassifier \cf6 .fit, training time - about 60 seconds \cf0 **\
selector = SelectPercentile(f_classif, percentile=\cf6 1\cf0 ) ** percentile=\cf6 1\cf5  \cf14 ** \cf2 dropped \cf6 .fit, training time from about 60 seconds to about 4 seconds\cf14  **\
\
\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl400\partightenfactor0
\ls1\ilvl0
\f7\fs30 \cf15 {\listtext	\'95	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec15 Change 
\f8 percentile
\f7  from 10 to 1.
\f1\fs22 \cf14 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl400\partightenfactor0
\ls2\ilvl0
\f7\fs30 \cf15 {\listtext	\'95	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec15 What\'92s the number of features now?\
\pard\pardeftab720\partightenfactor0

\f1\fs22 \cf14 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 \
\pard\pardeftab720\partightenfactor0
\cf2 selector = SelectPercentile(f_classif, \cf6 percentile=10\cf2 ), \cf6 percentile=10 down, train time down, features down from \cf2 3785 to 379 \cf14 \
\cf0 selector = SelectPercentile(f_classif, \cf6 percentile=10\cf0 ) 
\fs48 \cf6 -
\fs22 \cf0  \cf6 len\cf0 (features_test[0]) - \cf6 3785\cf14 \
\cf0 selector = SelectPercentile(f_classif, \cf6 percentile=1\cf0 ) 
\fs48 \cf6 -
\fs22 \cf0  len(features_test[0]) - \cf6 379\
\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl400\partightenfactor0
\ls3\ilvl0
\f7\fs30 \cf15 {\listtext	\'95	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec15 Would a large value for percentile lead to a more complex or less complex decision tree, all other things being equal? -\cf2  my guess more complex\cf15 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 	\
\pard\tx720\pardeftab720\sl400\partightenfactor0
\cf15 \
How many features are in the data -\
\pard\pardeftab720\partightenfactor0

\f1\fs22 \cf0 myDecisionTreeClassifier = sklearn.tree.DecisionTreeClassifier(min_samples_split = \cf5 40\cf0 ) ** \cf2 so far stayed the same\cf0  **\
\pard\tx720\pardeftab720\sl400\partightenfactor0

\f7\fs30 \cf15 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl400\partightenfactor0
\ls4\ilvl0\cf0 {\listtext	\'95	}\expnd0\expndtw0\kerning0
What\'92s the accuracy when percentile = 10? \cf2 features - \cf6 3785\cf0 \uc0\u8232 
\f1\fs22 \kerning1\expnd0\expndtw0 selector = SelectPercentile(f_classif, percentile=\cf5 10\cf0 )\
{\listtext	\'95	}myDecisionTreeClassifier\cf6 Accuracy\cf0  - 0.9772468714448237\
\pard\tx720\pardeftab720\sl400\partightenfactor0

\f7\fs30 \cf0 \
\pard\pardeftab720\partightenfactor0

\f1\fs22 \cf0 myDecisionTreeClassifier = sklearn.tree.DecisionTreeClassifier(min_samples_split = \cf5 40\cf0 ) ** \cf2 so far stayed the same\cf0  **
\f7\fs30 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl400\partightenfactor0
\ls5\ilvl0\cf0 {\listtext	\'95	}\expnd0\expndtw0\kerning0
What\'92s the accuracy when percentile = 1? \cf2 features - \cf6 379\cf0 \
\ls5\ilvl0\kerning1\expnd0\expndtw0 {\listtext	\'95	}
\f1\fs22 myDecisionTreeClassifier\cf6 Accuracy\cf0  - 0.9670079635949943\
\pard\tx720\pardeftab720\sl400\partightenfactor0
\cf0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardeftab720\pardirnatural\partightenfactor0

\f0\fs24 \cf0 ***\
*** 
\fs28 \cf8 \expnd0\expndtw0\kerning0
Introduction to Machine Learning
\fs24 \cf0 \kerning1\expnd0\expndtw0  - \cf3 Choose Your Own Algorithm\cf0  - 10 of 10 - 12 March 2017 \
***\
\
\pard\pardeftab720\partightenfactor0

\f1\fs22 \cf0 myKNeighborsClassifier = sklearn.neighbors.KNeighborsClassifier()\
myKNeighborsClassifier.\cf6 fit\cf0 (features_train,labels_train) ** train with (features_train,labels_train) **\
pred = myKNeighborsClassifier.\cf6 predict\cf0 (features_test)\
myKNeighborsClassifier_score = myKNeighborsClassifier.\cf6 score\cf0 (features_test,labels_test) \
myKNeighborsClassifier_score - 0.92\
\
*** functions associated with generating, drawing the graphs, plots ***\
\cf2 class_vis.py\
def\cf0  prettyPicture(clf, X_test, y_test):\
\cf2 def\cf0  output_image(name, \ul format\ulnone , \ul bytes\ulnone ):\
\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardeftab720\pardirnatural\partightenfactor0

\f0\fs24 \cf0 ***\
*** 
\fs28 \cf8 \expnd0\expndtw0\kerning0
Introduction to Machine Learning
\fs24 \cf0 \kerning1\expnd0\expndtw0  - \cf3 Datasets and Questions \cf0 - 35 of 35 - 12 March 2017 \
*** 
\fs28 \cf8 \expnd0\expndtw0\kerning0
Introduction to Machine Learning
\fs24 \cf0 \kerning1\expnd0\expndtw0  - \cf3 Datasets and Questions \cf0 - 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11 of 35 - 12 March 2017 \
***\
\

\fs28 \cf6 \expnd0\expndtw0\kerning0
git clone\cf8  https://github.com/udacity/ud120-projects\
\
accuracy - more data better than a finely tuned algorithm \
accuracy - more data better result than a super fine tuned algorithm \
\pard\tx720\pardeftab720\sl400\partightenfactor0
\cf8 poi_names.txt\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f3\fs22 \cf16 \kerning1\expnd0\expndtw0 \CocoaLigature0 $ \cf0 pwd\
/Users/Menfi/Documents/junk/enronData\
\cf16 \

\fs26 \cf6 y, n
\fs22 \cf16  \cf14 actually have the inbox\cf16  \
$ \cf0 head poi_names.txt \
http://usatoday30.usatoday.com/money/industries/energy/2005-12-28-enron-participants_x.htm\
\
(y) Lay, Kenneth\
(y) Skilling, Jeffrey\
(n) Howard, Kevin\
(n) Krautz, Michael\
(n) Yeager, Scott\
(n) Hirko, Joseph\
(n) Shelby, Rex\
(n) Bermingham, David\
\pard\tx720\pardeftab720\sl400\partightenfactor0

\f0\fs28 \cf8 \expnd0\expndtw0\kerning0
\CocoaLigature1 \
\ul Enron Data From The Web\ulnone \
https://www.cs.cmu.edu/~./enron/\
\cf2 https://www.cs.cmu.edu/~./enron/enron_mail_20150507.tgz\cf8 \
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f3\fs22 \cf0 \kerning1\expnd0\expndtw0 \CocoaLigature0 ls -l /Users/Menfi/Downloads/enron_mail_20150507\cf6 .tgz\cf0 \
-rw-r--r--@ 1 Menfi  staff  443254787 Mar 12 05:04 /Users/Menfi/\cf6 Downloads\cf0 /enron_mail_20150507.\cf6 tgz\cf0 \
\pard\tx720\pardeftab720\sl400\partightenfactor0

\f0\fs28 \cf8 \expnd0\expndtw0\kerning0
\CocoaLigature1 \
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f3\fs22 \cf16 \kerning1\expnd0\expndtw0 \CocoaLigature0 $ \cf6 gunzip\cf0  enron_mail_20150507\cf6 .tgz \cf0 \
\cf16 $ \cf0 ls -l\
-rw-r--r--  1 Menfi  staff  1823016960 Mar 12 05:04 enron_mail_20150507\cf6 .tar\cf0 \
\
$ ls -l /Users/Menfi/Documents/junk/enronData/one/\
-rw-r--r--  1 Menfi  staff  1823016960 Mar 12 05:04 enron_mail_20150507\cf6 .tar\cf0 \
\
/Users/Menfi/Documents/junk/enronData/one/expand/maildir - ** \cf2 Enron data here\cf0  **\
\
\
\pard\pardeftab720\partightenfactor0

\f0\fs28 \cf14 \ul \ulc14 \CocoaLigature1 Types of Data\
\ulnone numerical - numerical values - numbers \
categorical - limited number of discrete values - gender \
time series - temporal value (date, timestamp)\
text - words\cf0 \CocoaLigature0 \
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf0 \
salary - numerical\
job title - categorical \
timestamps on emails - time series \
contents of emails - text 	\
number of emails sent by a given person - numerical \
to / from fields of emails - text \
\pard\pardeftab720\partightenfactor0

\f1\fs22 \cf0 \CocoaLigature1 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardeftab720\pardirnatural\partightenfactor0

\f0\fs24 \cf0 ***\
*** 
\fs28 \cf8 \expnd0\expndtw0\kerning0
Introduction to Machine Learning
\fs24 \cf0 \kerning1\expnd0\expndtw0  - \cf3 Datasets and Questions \cf0 - Datasets and Questions Mini-Project 12 of 35 - 12 March 2017 \
***
\fs28 \cf8 \expnd0\expndtw0\kerning0
 \
\
\pard\pardeftab720\partightenfactor0

\f1\fs22 \cf2 \kerning1\expnd0\expndtw0 import\cf0  pickle
\f0\fs28 \cf8 \expnd0\expndtw0\kerning0
\

\f1\fs22 \cf0 \kerning1\expnd0\expndtw0 enron_data = pickle.load(open(\cf4 "../final_project/final_project_dataset.pkl"\cf0 , \cf4 "\cf4 \ul \ulc4 rb\cf4 \ulnone "\cf0 ))\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardeftab720\pardirnatural\partightenfactor0

\f0\fs28 \cf8 \expnd0\expndtw0\kerning0
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1\fs22 \cf0 \kerning1\expnd0\expndtw0 \
\
\pard\pardeftab720\partightenfactor0

\f0\fs28 \cf8 \expnd0\expndtw0\kerning0
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf8 \
\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\fs36 \cf6  
\fs28 \cf8 \
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf6 \
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardeftab720\pardirnatural\partightenfactor0
\cf8 \
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1\fs22 \cf0 \kerning1\expnd0\expndtw0 \
\pard\pardeftab720\partightenfactor0
\cf0 \
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f0\fs24 \cf0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\pard\pardeftab720\sl280\partightenfactor0
\cf0 \
\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\
\
\
aaa}