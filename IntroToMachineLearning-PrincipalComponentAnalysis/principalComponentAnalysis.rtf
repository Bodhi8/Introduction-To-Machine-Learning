{\rtf1\ansi\ansicpg1252\cocoartf1404\cocoasubrtf470
{\fonttbl\f0\froman\fcharset0 Times-Roman;\f1\fnil\fcharset0 Monaco;}
{\colortbl;\red255\green255\blue255;\red255\green0\blue0;\red0\green0\blue255;\red255\green0\blue255;
\red0\green170\blue0;\red128\green0\blue0;}
\margl1440\margr1440\vieww37080\viewh13580\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs36 \cf0 \
\
Principal Component Analysis - PCA    \
1.) Systematic way of transforming input features into principal components.\
2.) Use Principal Components (size, and neighborhood below) as \cf2 new features\cf0 . Use the new features INSTEAD of the originals.\
3.) Principal Components are directions in data that maximize variance, minimize information loss, when you project / compression down into them.\
4.) More variance of data along a Principal Component, the higher that Principal Component is ranked. Most variance - first Principal Component. \
5.) Principal Components are all perpendicular to each other, second  Principal Component is mathematically guaranteed to not overlap at all with the first Principal Component.\
most variance - most information - first Principal Component\
second most variance - second most information - second Principal Component (\cf3 no overlap with the first Principal Component\cf0 )\
therefore Principal Components are independent features  \
maximum number of Principal Components = number of input features (\cf3 features NOT training points\cf0 ) \
\
Feature Set Compression \
\
Given features of a home predict price - algorithm - Linear Regression  \
\pard\pardeftab720\partightenfactor0

\f1\fs22 \cf0 \

\f0\fs36 Principal Component of a Dataset is the direction that has the largest variance \
Think of an extended oval 1 line longer than the other line.\
\
Using Principal Component Analysis - PCA - digest down two features \
Safety Problems		PCA\
					---------> 		Neighborhood Quality\
School Ranking \
\
4 features split into 2 categories\
\
1.) square footage			PCA\cf3  1 of 2\cf0  PCA efforts\
						--------->	1.) size (\cf2 latent feature\cf0 )\
2.) number of rooms\
\
***\
\
3.) neighborhood safety		PCA \cf3 2 of 2\cf0  PCA efforts\
						--------->	2.) neighborhood (\cf2 latent feature\cf0 )\
4.) school quality\
\
The 2 categories are put into a regression - come up with house price \
\
***\
\cf3 Second PCA Strategy\cf0 \
\cf3 Place \cf2 ALL\cf3  features\cf0  (\cf3 1 PCA effort, not 2\cf0 ) in this case 4, would be different for facial recognition - millions of pixels\
\cf3 into PCA together \cf0 (\cf3 1 PCA effort, not 2\cf0 ).  \
Automatically combine features into new features, then rank the power of the new features.\
Powerful unsupervised learning technique.   \
\
Maximum Number of Principal Components\
What is the maximum number of principal components allowed by sklearn\
if you have a dataset with \cf3 100\cf0  training points and \cf3 4\cf0  features for each point? 4 the \cf2 MINIMUM\cf0  of the two \
\
*** Introduction To machine Learning - When to Use PCA - 29 of 38 6 April 2017***\
\cf3 \ul \ulc3 When to use PCA\
\cf0 \ulnone Access latent features driving the patterns in the data - size versus either number of rooms or square feet\
Achieve dimensionality reduction, visualize high-dimension data - reduce noise \
Use PCA as preprocessing before using another algorithm.\
Make other (downstream) algorithms (regression, classification) work better because fewer inputs (eigenfaces).  \

\f1\fs22 \

\fs36 Introduction To Machine Learning - Principal Component Analysis - Explained Variance of Each Principal Component\
How much of the variance is explained by the first Principal Component? 0.193\
How much of the variance is explained by the second Principal Component? 0.151\
\
pca = RandomizedPCA(n_components=n_components, whiten=\cf3 True\cf0 ).fit(X_train) # used in the course work\
\pard\pardeftab720\partightenfactor0
\cf4 # \ul pca\ulnone  = PCA(svd_solver = 'randomized', whiten = True).fit(X_train) \cf0 # not used in the course work\
\pard\pardeftab720\partightenfactor0
\cf3 print\cf0 (\cf5 "\ul pca\ulnone  - \{\}"\cf0 .format(pca))\
\pard\pardeftab720\partightenfactor0
\cf4 # \ul pca\ulnone  - RandomizedPCA(copy=True, iterated_power=2, n_components=150, random_state=None, whiten=True) \cf0 # used in the course work\
\cf4 # \ul pca\ulnone  - PCA(copy=True, iterated_power='auto', n_components=None, random_state=None, svd_solver='randomized', \ul tol\ulnone =0.0, whiten=True) \cf0 # not used in the course work\
\pard\pardeftab720\partightenfactor0
\cf3 print\cf0 (\cf5 "type(\ul pca\ulnone ) - \{\}\\n"\cf0 .format(type(pca)))\
\pard\pardeftab720\partightenfactor0
\cf4 # type(\ul pca\ulnone ) - <class 'sklearn.decomposition.pca.RandomizedPCA'> \cf0 # used in the course work\
\cf4 #        type(\ul pca\ulnone ) - <class 'sklearn.decomposition.pca.PCA'> \cf0 # not used in the course work\
\
\pard\pardeftab720\partightenfactor0
\cf3 print\cf0 (\cf5 'pca.explained_variance_ratio_'\cf0 )\
\cf3 print\cf0 (pca.explained_variance_ratio_)\
pca.explained_variance_ratio_\
[ \cf2 0.19346364  0.15116846\cf0   0.07083635  0.05952002  0.05157537  0.02887196\
\cf3 print\cf0 ()\
\
\cf3 print\cf0 (\cf5 'pca.explained_variance_ratio_[0]'\cf0 )\

\fs22 pca.explained_variance_ratio_[0]\

\fs36 \cf3 print\cf0 (pca.explained_variance_ratio_[\cf6 0\cf0 ])\
\pard\pardeftab720\partightenfactor0

\fs22 \cf2 0.193463642815
\fs36 \
\pard\pardeftab720\partightenfactor0
\cf3 print\cf0 (\cf5 'pca.explained_variance_ratio_[1]'\cf0 )\

\fs22 pca.explained_variance_ratio_[1]
\fs36 \
\cf3 print\cf0 (pca.explained_variance_ratio_[\cf6 1\cf0 ])\
\pard\pardeftab720\partightenfactor0

\fs22 \cf2 0.151168464426
\fs36 \cf0 \
\
Introduction To Machine Learning - Principal Component Analysis - Selecting A Number of Principal Components\
What is a good way to figure out how many Principal Components to use?\
Train on different number of Principal Components (n-components) and see how accuracy responds.\
Cut off when it becomes apparent that adding more Principal Components does not buy you more discrimination.\
\
\
\
\
\
\
\
\
\pard\pardeftab720\partightenfactor0

\f0 \cf0 \
\
\
\
\
 \
\
\
\
\
}